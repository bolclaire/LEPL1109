{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **[LEPL1109] - STATISTICS AND DATA SCIENCES**\n",
    "## **Hackaton 02 - Classification: Diabetes Health indicators**\n",
    "\\\n",
    "Prof. D. Hainaut\\\n",
    "Prod. L. Jacques\\\n",
    "\\\n",
    "\\\n",
    "Adrien Banse (adrien.banse@uclouvain.be)\\\n",
    "Jana Jovcheva (jana.jovcheva@uclouvain.be)\\\n",
    "François Lessage (francois.lessage@uclouvain.be)\\\n",
    "Sofiane Tanji (sofiane.tanji@uclouvain.be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figures/diab_illustration.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<b>[IMPORTANT] Read all the documentation</b>  <br>\n",
    "    Make sure that you read the whole notebook, <b>and</b> the <code>README.md</code> file in the folder.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Guidelines and Deliverables**\n",
    "\n",
    "*   This hackaton is due on the **29 November 2024 at 23h59**\n",
    "*   Copying code or answers from other groups (or from the internet) is strictly forbidden. <b>Each source of inspiration (stack overflow, git, other groups, ChatGPT...) must be clearly indicated!</b>\n",
    "*  This notebook (with the \"ipynb\" extension) file, the Python source file (\".py\"), the report (PDF format) and all other files that are necessary to run your code must be delivered on <b>Moodle</b>.\n",
    "* Only the PDF report and the python source file will be graded, both on their content and the quality of the text / figures.\n",
    "  * 4/10 for the code.\n",
    "  * 4/10 for the Latex report.\n",
    "  * 2/10 for the vizualisation. <br><br>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[DELIVERABLE] Summary</b>  <br>\n",
    "After the reading of this document (and playing with the code!), we expect you to provide us with:\n",
    "<ol>\n",
    "   <li> a PDF file (written in LaTeX, see example on Moodle) that answers all the questions below. The report should contain high quality figures with named axes (we recommend saving plots with the <samp>.pdf</samp> extension);\n",
    "   <li> a Python file with your classifier implementation. Please follow the template that is provided and ensure it passes the so-called <i>sanity</i> tests;\n",
    "   <li> this Jupyter Notebook (it will not be read, just checked for plagiarism);\n",
    "   <li> and all other files (not the datasets!) we would need to run your code.\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "As mentioned above, plagiarism is forbidden. However, we cannot forbid you to use artificial intelligence BUT we remind you that the aim of this project is to learn classification on your own and with the help of the course material. Finally, we remind you that for the same question, artificial intelligence presents similar solutions, which could be perceived as a form of plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Context & Objective**\n",
    "Diabetes is among the most prevalent chronic diseases in the United States, impacting millions of Americans each year and exerting a significant financial burden on the economy. Diabetes is a serious chronic disease in which individuals lose the ability to effectively regulate levels of glucose in the blood, and can lead to reduced quality of life and life expectancy. After different foods are broken down into sugars during digestion, the sugars are then released into the bloodstream. This signals the pancreas to release insulin. Insulin helps enable cells within the body to use those sugars in the bloodstream for energy. Diabetes is generally characterized by either the body not making enough insulin or being unable to use the insulin that is made as effectively as needed.\\\n",
    "Complications like heart disease, vision loss, lower-limb amputation, and kidney disease are associated with chronically high levels of sugar remaining in the bloodstream for those with diabetes. While there is no cure for diabetes, strategies like losing weight, eating healthily, being active, and receiving medical treatments can mitigate the harms of this disease in many patients. Early diagnosis can lead to lifestyle changes and more effective treatment, making predictive models for diabetes risk important tools for public and public health officials.\n",
    "\n",
    "You work in the diabetology department at **Saint Luc University Hospital**. The head of the department has asked you to find a solution for classifying and predicting **whether patients are at high risk of developing diabetes**. This will enable them to schedule an appointment with these patients to set up prevention tools. To do this, you have a database of patients who have passed through the department in recent years. In addition, the head of the department feels that the poll is too long, and would like to **reduce the number of questions while maintaining the reliability and quality of the results**.\\\n",
    "Your aim is to determine which characteristics are relevant and enable reliable patient classification. Be careful, don’t let a potential diabetic patient slip through the cracks. The rest of this document will guide you in this process.\n",
    "\n",
    "## **Dataset description**\n",
    "\n",
    " \n",
    "The data set is a real-world data set based on a survey (BRFSS) conducted by the Centers for Disease Control and Prevention in the USA some ten years ago.\\\n",
    "The Behavioral Risk Factor Surveillance System (BRFSS) is an annual telephone health survey conducted by the Centers for Disease Control and Prevention. Each year, the survey collects responses from over 400,000 Americans on health-related risk behaviors, chronic diseases and use of preventive services. The survey has been conducted annually since 1984. It contains 22 headings and around 70,000 entries.\n",
    "\n",
    "\n",
    "<img src=\"figures/Features_table.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "## **Notebook structure**\n",
    "\n",
    "* PART 1 - Preliminaries\n",
    "   - 1.1 - Importing the packages\n",
    "   - 1.2 - Importing the dataset\n",
    "   - 1.3 - Is the dataset balanced?\n",
    "   - 1.4 - Scale the dataset\n",
    "    <br><br>\n",
    "* PART 2 - Correlation\n",
    "   - 2.1 - Correlation matrix \n",
    "   - 2.2 -Analyze the correlation with diabetes\n",
    "   - 2.3 - Model selection and parameters tuning\n",
    "   - 2.4 - Precision-Recall curve and thresholding\n",
    "   <br><br>\n",
    "* PART 3 - Classifiers\n",
    "   - 3.1 - Linear regressor\n",
    "   - 3.2 - Logisitic regressor\n",
    "   - 3.3 - KNN regressor\n",
    "   <br><br>\n",
    "* PART 4 - Validation metrics\n",
    "   - 4.1 - Precision score\n",
    "   - 4.2 - Recall score\n",
    "   - 4.3 - F1 score\n",
    "   <br><br>\n",
    "* PART 5 - Reduce the questionnaire size\n",
    "   - 5.1 - K-Fold preparation\n",
    "   - 5.2 - Find the right combination length/regressor\n",
    "   - 5.3 - Visualize the scores\n",
    "   <br><br>   \n",
    "* PART 6 - Visualization\n",
    "   - 6.1 - Visualize your results\n",
    "\n",
    "We filled this notebook with preliminary (trivial) code. This practice makes possible to run each cell, even the last ones, without throwing warnings. <b>Take advantage of this aspect to divide the work between all team members!</b> <br><br>\n",
    "Remember that many libraries exist in Python, so many functions have already been developed. Read the documentation and don't reinvent the wheel! You can import whatever you want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART I - Preliminaries</b> </font> <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the hackathon, we will import the necessary packages, then we will import the dataset, scale it and analyze its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.1 : IMPORTING ALL THE NECESSARY PACKAGES\n",
    "\n",
    "@pre:  /\n",
    "@post: The necessary packages should be loaded.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Import all the necessary packages here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.2 : IMPORTING THE DATASET\n",
    "\n",
    "@pre:  /\n",
    "@post: The object `df` should contain a Pandas DataFrame corresponding to the file `diabetes_dataset.csv`\n",
    "\"\"\"\n",
    "\n",
    "file = \"diabetes_dataset.csv\"\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "df = pd.DataFrame(df) # To modify\n",
    "df2 = pd.DataFrame(df) # To keep the original df\n",
    "df\n",
    "# df.describe()\n",
    "# df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Is the dataset balanced?***\n",
    "\n",
    "It's good practice to check this to better understand the contents of our dataset. The balance between the different classes has an impact on the binarization threshold (which is initialized here at 0.5). Other things can also have an impact on the choice of threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.3 : IS THE DATASET BALANCED?\n",
    "\n",
    "@pre:  `df` contains the dataset\n",
    "@post: Plot the diabetic/non-diabetic distribution in a pie chart\n",
    "\"\"\"\n",
    "\n",
    "# Plot the diabetic/non-diabetic distribution in a pie chart here...\n",
    "\n",
    "df['Diabetes'].value_counts().plot.pie(autopct='%1.1f%%')\n",
    "# print(df['Diabetes'].value_counts())\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Standardize*** is important when you work with data because it allows data to be compared with one another. \n",
    "\n",
    "$z$ is the standard score of a population $x$. It can be computed as follows:\n",
    "$$z = \\frac{x-\\mu}{\\sigma}$$\n",
    "with $\\mu$ the mean of the population and $\\sigma$ the standard deviation of the poplutation.\n",
    "\n",
    "Please consult, [Wikipedia](https://en.wikipedia.org/wiki/Standard_score) for further information about the standardization.\\\n",
    "Be careful to use the same formula as us, check in `scikit-learn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.4 : SCALE THE DATASET\n",
    "\n",
    "@pre:  A pandas.DataFrame `df` containing the dataset\n",
    "@post: A pandas.DataFrame `df` containing the standardized dataset (except classification columns (Diabetes))\n",
    "\n",
    "\"\"\"\n",
    "from sklearn import preprocessing\n",
    "def scale_dataset(df): \n",
    "    # Modify here...\n",
    "    columns = df.iloc[:, 1:].columns\n",
    "    Scale = preprocessing.StandardScaler()\n",
    "    df[columns] = Scale.fit_transform(df[columns])\n",
    "    return df , Scale\n",
    "\n",
    "df , Scale = scale_dataset(df)\n",
    "df\n",
    "# df.info()\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART II - Correlation</b> </font> <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In order to keep*** the important features for our classification, we can compute and plot (see e.g. `seaborn.heatmap`) the correlation matrix. With these correlation coefficient, we can establish a feature selection strategy.\\\n",
    "Be sure to use the `pearson` correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\"\"\"\n",
    "CELL N°2.1 : CORRELATION MATRIX\n",
    "\n",
    "@pre:  `df` contains the diabetes dataset\n",
    "@post: `corr_matrix` is a Pandas DataFrame that contains the correlation matrix of the full dataset\n",
    "\"\"\"\n",
    "\n",
    "corr_matrix = df.corr(method='pearson')\n",
    "seaborn.heatmap(corr_matrix,cmap='coolwarm',vmin=-1,vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this visualization, it is time to sort the coefficients of correlation to keep them with the best correlation with `Diabetes`. **Be careful** with the sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°2.2 : ANALYZE THE CORRELATION WITH DIABETE\n",
    "\n",
    "@pre:  `corr_matrix` is a Pandas DataFrame that contains the correlation matrix of the training set\n",
    "@post: `sorted_features` contains a list of features (columns of `df`) \n",
    "       sorted according to their correlation with `Diabetes` \n",
    "\"\"\"\n",
    "\n",
    "def sort_features(corr_matrix):\n",
    "    corr = corr_matrix[\"Diabetes\"].drop(\"Diabetes\")\n",
    "    return corr.sort_values(key=abs,ascending=False).axes[0].to_list()\n",
    "sorted_features = sort_features(corr_matrix)\n",
    "print(sorted_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART III - Classifiers</b> </font> <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third part, you need to write functions that return a lamba function with a classifier for the test set. **Be careful** to keep the same form as the one suggested to pass the sanity checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement** the *linear_regressor*. Please follow the specifications in the provided template.\n",
    "\n",
    "**Reminder:** Linear regressor is a model that predicts a continuous value by fitting a line (or hyperplane) to the data, minimizing the difference between observed and predicted values.\n",
    "\n",
    "Please consult, [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression) for further information about the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3.1 : LINEAR REGRESSOR\n",
    "\n",
    "@pre:  `X_train` and `y_train` contain the training set of `df` and a threshold that is a numerical value (float) by default 0.5.\n",
    "@post: Lambda function that takes `X_test` and returns a 1D array of binary predictions (0 or 1) according to the given threshold.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def linear_regressor(X_train, y_train, threshold = 0.5):\n",
    "    # To modify\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return lambda X_test: (model.predict(X_test) > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement** the *logistic_regressor*. Please follow the specifications in the provided template.\n",
    "\n",
    "**Reminder:** Logisitic regressor is a classification model that estimates the probability of an observation belonging to a class using a logistic function; suitable for binary (and multiclass) problems.\n",
    "\n",
    "Please consult, [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression) for further information about the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3.2 : LOGISTIC REGRESSOR\n",
    "\n",
    "@pre:  `X_train` and `y_train` contain the training set of `df` and a threshold that is a numerical value (float) by default 0.5.\n",
    "@post:  Lambda function that takes `X_test` and returns a 1D array of binary predictions (0 or 1) according to the given threshold.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regressor(X_train, y_train, threshold = 0.5):\n",
    "    # To modify\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return lambda X_test: (model.predict(X_test) > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement** the *knn_regressor*. Please follow the specifications in the provided template.  <br>\n",
    "\n",
    "**Reminder:** Knn regressor is a non-parametric classification algorithm that classifies an observation according to the classes of its k nearest neighbors in feature space.\n",
    "\n",
    "Please consult, [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) for further information about the classifier.\\\n",
    "Attention, you must implement it with **Euclidian distance** and **10** neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3.3 : KNN REGRESSOR\n",
    "\n",
    "@pre:  `X_train` and `y_train` contain the training set of `df` and a threshold that is a numerical value (float) by default 0.5.\n",
    "@post: Lambda function that takes `X_test` and returns a 1D array of binary predictions (0 or 1) according to the given threshold.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn_regressor(X_train, y_train, threshold = 0.5, n_neighbors = 10):\n",
    "    # To modify\n",
    "    model = KNeighborsClassifier(n_neighbors=n_neighbors, metric='euclidean')\n",
    "    model.fit(X_train, y_train)\n",
    "    return lambda X_test: model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART IV - Validation metrics</b> </font> <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will implement tools that will help us to **validate** the prediction models implemented in Part III. In particular, we will use the _precision, recall_ and _F1 score_ metrics. \n",
    "\n",
    "**Implement** the _precision, recall_ and _F1 score_. Please follow the specifications in the provided template.  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**\n",
    "\n",
    "$F_1$ is a performance metric allowing to obtain some trade-off between the precision and recall criterions. It can be computed as follows:\n",
    "$$F_1 = 2~\\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}.$$\n",
    "\n",
    "Please consult, [Wikipedia](https://en.wikipedia.org/wiki/F-score) for further information about the three metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°4.1 : PRECISION SCORE\n",
    "\n",
    "@pre:  /\n",
    "@post: `precision(y_test, y_pred)` returns the prediction metric based on the predicted labels `y_pred`\n",
    "       and the true labels `y_test`. \n",
    "\"\"\"\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def precision(y_test, y_pred):\n",
    "    \n",
    "    return precision_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°4.2 : RECALL SCORE\n",
    "\n",
    "@pre:  /\n",
    "@post: `recall(y_test, y_pred)` returns the recall metric based on the predicted labels `y_pred`\n",
    "       and the true labels `y_test`. \n",
    "\"\"\"\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def recall(y_test, y_pred):\n",
    "    \n",
    "    return recall_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°4.3 : F1 SCORE\n",
    "\n",
    "@pre:  /\n",
    "@post: `f1_score(y_test, y_pred)` returns the F1 score metric based on the predicted labels `y_pred`\n",
    "       and the true labels `y_test`. \n",
    "\"\"\"\n",
    "from sklearn.metrics import f1_score as f1\n",
    "def f1_score(y_test, y_pred):\n",
    "    \n",
    "    return f1(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART V - Reduce the questionnaire size</b> </font> <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, find a model that satisfies the following specifications: \n",
    "- A recall of at least 95%\n",
    "- A F1 score of at least 75%\n",
    "\n",
    "For that, we will use **k-fold** cross validation (see [the Wikipedia page](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation) for a reminder), and then test the three models above with \n",
    "- Different number of features\n",
    "- Different thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use k-fold cross validation, use the class `sklearn.model_selection.\n",
    "old` from the `scikit-learn` library (see [the documentation](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.KFold.html)) with `n_splits = 3`.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>[IMPORTANT] Grading</b>  <br>\n",
    "In order for us to be able to automatically grade your submission, put <code>shuffle=True</code>, and <code>random_state=1109</code> when you initialize <code>KFold</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°5.1 : K-FOLD PREPARATION\n",
    "\n",
    "@pre:  `df` contains the scaled dataset.\n",
    "@post: The following specifications should be satisfied: \n",
    "            - `kf` should contain a `KFold` object with 3 splits, shuffled and with 1109 seed. \n",
    "            - `X` should contain a pd.DataFrame with all the features (all columns except `Diabetes`)\n",
    "            - `y` should contain a pd.DataFrame with all the labels (only the column `Diabetes`)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize KFold with 3 splits, shuffle=True, and random_state=1109\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1109)\n",
    "\n",
    "# Separate the features (X) and labels (y) from the dataset\n",
    "X = df.drop(columns=[\"Diabetes\"])  # All columns except 'Diabetes' are features\n",
    "y = df[\"Diabetes\"]  # The 'Diabetes' column is the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find our model, proceed as follows: \n",
    "- Fix a threshold in $(0, 1)$\n",
    "- Define a dictionary `result` of the form \n",
    "\n",
    "<code>result = {\n",
    "    \"linear\": {}, \n",
    "    \"logistic\": {}, \n",
    "    \"knn\": {}\n",
    "}\n",
    "</code>\n",
    "\n",
    "- For $ i \\in \\{1, \\dots, \\texttt{N\\_features}\\} $: \n",
    "    - Select the $i$ **most correlated features** (use `sorted_features` defined above)\n",
    "    - For all the pairs $((X_{\\text{train}}, y_{\\text{train}}), (X_{\\text{test}}, y_{\\text{test}}))$ given by k-fold\n",
    "        - Compute the linear, logistic and KNN regressors with the fixed threshold on $X_{\\text{train}}$\n",
    "        - Compute the 3 different 3-tuple `validation(regressor, X_test, y_test)`\n",
    "    - In `result[reg][i]`, save the **average** of all the validation tuples you computed for `reg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "def validation(regressor, X_test, y_test):\n",
    "    # Nothing to do here!\n",
    "    y_pred = regressor(X_test)\n",
    "    return (\n",
    "        recall_score(y_test, y_pred), \n",
    "        precision_score(y_test, y_pred), \n",
    "        f1_score(y_test, y_pred)\n",
    "    )\n",
    "\n",
    "# Fix the threshold\n",
    "threshold = 0.29\n",
    "\n",
    "# Initialize the result dictionary\n",
    "result = {\n",
    "    \"linear\": {}, \n",
    "    \"logistic\": {}, \n",
    "    \"knn\": {}\n",
    "}\n",
    "\n",
    "# Assuming `sorted_features` contains the features sorted by correlation\n",
    "# This variable should be defined before using it\n",
    "N_features = len(X.columns)\n",
    "\n",
    "# Iterate over the number of features to keep\n",
    "for i in range(1, N_features + 1):\n",
    "    # Select the `i` most correlated features\n",
    "    selected_features = sorted_features[:i]\n",
    "    \n",
    "    # Initialize lists to store validation metrics for each regressor\n",
    "    linear_metrics = []\n",
    "    logistic_metrics = []\n",
    "    knn_metrics = []\n",
    "    \n",
    "    # Perform k-fold cross-validation\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the dataset\n",
    "        X_train, X_test = X.iloc[train_index][selected_features], X.iloc[test_index][selected_features]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Convert to NumPy arrays\n",
    "        X_train_np, X_test_np = X_train.values, X_test.values\n",
    "        y_train_np, y_test_np = y_train.values, y_test.values\n",
    "        \n",
    "        # Linear regression\n",
    "        linear_pred = linear_regressor(X_train_np, y_train_np, threshold)(X_test_np)\n",
    "        linear_metrics.append(validation(lambda X: linear_pred, X_test_np, y_test_np))\n",
    "        \n",
    "        # Logistic regression\n",
    "        logistic_pred = logistic_regressor(X_train_np, y_train_np, threshold)(X_test_np)\n",
    "        logistic_metrics.append(validation(lambda X: logistic_pred, X_test_np, y_test_np))\n",
    "        \n",
    "        # KNN\n",
    "        knn_pred = knn_regressor(X_train_np, y_train_np, threshold)(X_test_np)\n",
    "        knn_metrics.append(validation(lambda X: knn_pred, X_test_np, y_test_np))\n",
    "    \n",
    "    # Compute the average of the validation metrics for each regressor\n",
    "    result[\"linear\"][i] = tuple(np.mean(metric) for metric in zip(*linear_metrics))\n",
    "    result[\"logistic\"][i] = tuple(np.mean(metric) for metric in zip(*logistic_metrics))\n",
    "    result[\"knn\"][i] = tuple(np.mean(metric) for metric in zip(*knn_metrics))\n",
    "\n",
    "# `result` now contains the average metrics for all combinations of regressors and feature counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell allows you to test if the threshold that you chose satisfies the specifications, that are \n",
    "- A recall of at least 95%\n",
    "- A F1 score of at least 75%\n",
    "\n",
    "Plot these graphs for different threshold, and select the model **with the smallest number of questions** that satisfy the conditions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°5.3 : VISUALIZE THE SCORES\n",
    "\n",
    "@pre:  `result` contains the average of the validations for regressor `reg`, when keeping the `i` most correlated features\n",
    "@post: plot of the scores for each condition\n",
    "\"\"\"\n",
    "\n",
    "# Nothing to do here, just run me! \n",
    "\n",
    "from helper import plot_result\n",
    "plot_result(result, threshold, to_show = \"recall\")\n",
    "plot_result(result, threshold, to_show = \"f1_score\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART VI - Visualization</b> </font> <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are asked to produce a **clear and clean figure** expressing a result\n",
    "or giving an overall vision of your work for this hackaton. **Please feel free to do as you\n",
    "wish. Be original!** \n",
    "\n",
    "The **clarity**, **content** and **description** (in the report) of your figure will be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "s = sorted_features[:5]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_confidence_from_metrics(precision, recall, total_samples, positive_rate):\n",
    "    \"\"\"\n",
    "    Calcule les pourcentages de TP, TN, FP et FN à partir de la précision, du rappel,\n",
    "    du nombre total d'échantillons et du taux de positifs réels.\n",
    "\n",
    "    :param precision: Précision du modèle (Precision)\n",
    "    :param recall: Rappel du modèle (Recall)\n",
    "    :param total_samples: Nombre total d'exemples (N)\n",
    "    :param positive_rate: Taux de positifs réels (P/N)\n",
    "    :return: Pourcentages de TP, TN, FP, FN\n",
    "    \"\"\"\n",
    "    P = positive_rate * total_samples\n",
    "    TP = recall * P\n",
    "    FP = TP * (1 / precision - 1)\n",
    "    FN = P - TP\n",
    "    TN = total_samples - TP - FP - FN\n",
    "\n",
    "    tp_percentage = (TP / total_samples) * 100\n",
    "    tn_percentage = (TN / total_samples) * 100\n",
    "    fp_percentage = (FP / total_samples) * 100\n",
    "    fn_percentage = (FN / total_samples) * 100\n",
    "\n",
    "    return tp_percentage, tn_percentage, fp_percentage, fn_percentage\n",
    "\n",
    "def plot_confidence_bars(tp, tn, fp, fn):\n",
    "    \"\"\"\n",
    "    Affiche un graphique en barres représentant les pourcentages de :\n",
    "    - Vrai positif (TP)\n",
    "    - Vrai négatif (TN)\n",
    "    - Faux positif (FP)\n",
    "    - Faux négatif (FN)\n",
    "\n",
    "    :param tp: Pourcentage de vrai positif\n",
    "    :param tn: Pourcentage de vrai négatif\n",
    "    :param fp: Pourcentage de faux positif\n",
    "    :param fn: Pourcentage de faux négatif\n",
    "    \"\"\"\n",
    "    categories = [\"Vrai Positif\", \"Vrai Négatif\", \"Faux Positif\", \"Faux Négatif\"]\n",
    "    percentages = [tp, tn, fp, fn]\n",
    "\n",
    "    # Configuration du graphique\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(categories, percentages, color=[\"green\", \"blue\", \"orange\", \"red\"], alpha=0.7)\n",
    "\n",
    "    # Ajouter les pourcentages au-dessus des barres\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2.0, height + 0.5, f\"{height:.1f}%\", ha='center', va='bottom')\n",
    "\n",
    "    plt.title(\"Pourcentage des prédictions\")\n",
    "    plt.ylabel(\"Pourcentage (%)\")\n",
    "    plt.ylim(0, 100)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Exemple d'utilisation avec des valeurs fictives\n",
    "recall, precision, f1_score = result['linear'][5]\n",
    "\n",
    "     # Exemple : rappel du modèle\n",
    "total_samples = 70692  # Exemple : nombre total d'échantillons\n",
    "positive_rate = 0.5   # Exemple : 30% de positifs réels\n",
    "\n",
    "# Calcul des pourcentages\n",
    "tp_percentage, tn_percentage, fp_percentage, fn_percentage = calculate_confidence_from_metrics(\n",
    "    precision, recall, total_samples, positive_rate\n",
    ")\n",
    "\n",
    "# Affichage du graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Exemple de données s pour les colonnes de df2\n",
    "s = ['GenHlth', 'HighBP', 'BMI', 'HighChol', 'Age']\n",
    "\n",
    "# Dictionnaire des questions\n",
    "dico = { \n",
    "    'GenHlth': \"À combien sur une échelle de 1 à 5 vous sentez-vous en bonne santé ?\",\n",
    "    'HighBP': \"Avez-vous une tension artérielle élevée ?\",\n",
    "    'BMI': \"Quel est votre indice de masse corporelle ?\",\n",
    "    'HighChol': \"Avez-vous un taux de cholestérol élevé ?\",\n",
    "    'Age': \"Dans quelle catégorie d'âge vous situez-vous ? (18-24, 25-34, 35-44, 45-54, 55-64, 65-74, 80+)\"\n",
    "}\n",
    "\n",
    "# Liste de questions du questionnaire\n",
    "questions = [dico[i] for i in s]\n",
    "\n",
    "# Liste de réponses possibles pour chaque question\n",
    "options = [\n",
    "    [\"1\", \"2\", \"3\", \"4\", \"5\"],\n",
    "    [\"oui\", \"non\"],\n",
    "    [],\n",
    "    [\"oui\", \"non\"],\n",
    "    ['18-24', '25-31', '32-38', '39-45', '46-52', '53-59', '60-66', '67-73', '74-80', '80+'],\n",
    "]\n",
    "\n",
    "# Stockage des widgets pour chaque question\n",
    "widgets_list = []\n",
    "\n",
    "# Créer la mise en page pour le questionnaire et le graphique\n",
    "questionnaire_box = widgets.VBox()\n",
    "graph_box = widgets.VBox()\n",
    "\n",
    "# Créer les widgets pour toutes les questions\n",
    "def create_questionnaire():\n",
    "    for i, question in enumerate(questions):\n",
    "        question_widget = widgets.HTML(value=f\"<h4>{i + 1}. {question}</h4>\")\n",
    "        questionnaire_box.children += (question_widget,)\n",
    "\n",
    "        if options[i]:  # Question avec choix multiples\n",
    "            dropdown = widgets.Dropdown(\n",
    "                options=options[i],\n",
    "                description='Réponse:'\n",
    "            )\n",
    "            widgets_list.append(dropdown)\n",
    "            questionnaire_box.children += (dropdown,)\n",
    "        else:  # Question ouverte\n",
    "            text_input = widgets.Text(\n",
    "                placeholder='Tapez votre réponse ici...',\n",
    "                description='Réponse:'\n",
    "            )\n",
    "            widgets_list.append(text_input)\n",
    "            questionnaire_box.children += (text_input,)\n",
    "\n",
    "    submit_button = widgets.Button(description=\"Soumettre\")\n",
    "    submit_button.on_click(on_submit)\n",
    "    questionnaire_box.children += (submit_button,)\n",
    "\n",
    "    # Afficher le questionnaire\n",
    "    display(questionnaire_box)\n",
    "\n",
    "# Gérer la soumission des réponses et afficher le graphique\n",
    "def on_submit(button):\n",
    "    answers = [widget.value for widget in widgets_list]\n",
    "    for i in range(len(answers)):\n",
    "        if answers[i] == 'oui':\n",
    "            answers[i] = 1\n",
    "        elif answers[i] == 'non':\n",
    "            answers[i] = 0\n",
    "        elif answers[i] in ['18-24', '25-31', '32-38', '39-45', '46-52', '53-59', '60-66', '67-73', '74-80', '80+']:\n",
    "            answers[i] = int(answers[i].split('-')[0])  # Conversion pour les tranches d'âge\n",
    "        elif answers[i].isdigit():\n",
    "            answers[i] = int(answers[i])\n",
    "        else:\n",
    "            answers[i] = int(answers[i].split(',')[0])\n",
    "\n",
    "    answers = np.array(answers).reshape(1, -1)\n",
    "\n",
    "    # Préparation des données pour la prédiction\n",
    "    X_train, y_train = df2[s], df2['Diabetes']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    answers = scaler.transform(answers)\n",
    "\n",
    "    # Modèle de régression linéaire\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(answers)\n",
    "\n",
    "    # Affichage de la prédiction\n",
    "    prediction_text = \"Vous avez un diabète, Attention veuillez considérer le fait que les résultats peuvent être érronés comme observé sur ce graphique\" if prediction > 0.29 else \"Vous n'avez pas de diabète, Attention veuillez considérer le fait que les résultats peuvent être érronés comme observé sur ce graphique\"\n",
    "    result_widget = widgets.HTML(value=f\"<h3>{prediction_text}</h3>\")\n",
    "    graph_box.children += (result_widget,)\n",
    "\n",
    "    # Affichage du graphique de confiance (utilisez votre fonction plot_confidence_bars)\n",
    "    plot_confidence_bars(tp_percentage, tn_percentage, fp_percentage, fn_percentage)\n",
    "    display(graph_box)\n",
    "\n",
    "# Créer et afficher le questionnaire\n",
    "create_questionnaire()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
