{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5  color=#003366> <b>[LEPL1109] - STATISTICS AND DATA SCIENCES</b> <br><br> \n",
    "<b>Hackathon 03 - Clustering: Bias in sensitive datasets</b> </font> <br><br><br>\n",
    "\n",
    "<font size=4  color=#003366>\n",
    "Prof. D. Hainaut<br>\n",
    "Prof. L. Jacques<br>\n",
    "\n",
    "<br><br>\n",
    "Adrien Banse (adrien.banse@uclouvain.be)<br>\n",
    "Jana Jovcheva (jana.jovcheva@uclouvain.be)<br>\n",
    "François Lessage (francois.lessage@uclouvain.be)<br>\n",
    "Sofiane Tanji (sofiane.tanji@uclouvain.be)<br>\n",
    "<div style=\"text-align: right\"> Version 2024-2025</div>\n",
    "<br><br>\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>[IMPORTANT] Read all the documentation</b>  <br>\n",
    "    Make sure that you read the whole notebook, <b>and</b> the <code>README.md</code> file in the folder.\n",
    "</div>\n",
    "<br><br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Guidelines and Deliverables**\n",
    "\n",
    "*   This hackathon is due on the **22 December 2024 at 22h00**\n",
    "*   Copying code or answers from other groups (or from the internet) is strictly forbidden. <b>Each source of inspiration (stack overflow, git, other groups, ChatGPT...) must be clearly indicated!</b>\n",
    "*  This notebook (with the \"ipynb\" extension) file, the report (PDF format) and all other files that are necessary to run your code must be delivered on <b>Moodle</b>.\n",
    "* Only the PDF report and the python source file will be graded, both on their content and the quality of the text / figures.\n",
    "  * 5/10 for the code.\n",
    "  * 4/10 for the Latex report.\n",
    "  * 1/10 for the visualization. <br><br>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[DELIVERABLE] Summary</b>  <br>\n",
    "After the reading of this document (and playing with the code!), we expect you to provide us with:\n",
    "<ol>\n",
    "   <li> a PDF file (written in LaTeX) that answers all the questions below. The report should contain high quality figures with named axes (we recommend saving plots with the <samp>.pdf</samp> extension);\n",
    "   <li> this Jupyter Notebook (it will be read, checked for plagiarism and evaluated);\n",
    "   <li> and all other files we would need to run your code.\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "As mentioned above, plagiarism is forbidden. However, we cannot forbid you to use artificial intelligence BUT we remind you that the aim of this project is to learn on your own and with the help of the course material. Finally, we remind you that for the same question, artificial intelligence presents similar solutions, which could be perceived as a form of plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Context & Objective**\n",
    "\n",
    "## Context\n",
    "\n",
    "Predictive algorithms serve multiple functions in criminal justice. They forecast crime locations, identify potential violent offenders, predict court appearance compliance, and estimate recidivism risk. \n",
    "COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) stands as a prominent risk assessment tool. Since 1998, the COMPAS risk score has been used by many jurisdictions in the United States to assess risk of recidivism in pre-trial bail decisions.\n",
    "In the United States, a defendant may either be detained or released on bail *(sous caution)* prior to the trial in court depending on various factors. Judges may detain defendants or increase the bail amount based on the risk score provided by the COMPAS algorithm.\n",
    "\n",
    "\n",
    "In 2016, investigative journalists at ProPublica published [Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), highlighting significant biases in the COMPAS Algorithm. Specifically, they showed that the proportion of false positives for African-American defendants is significantly higher than for Caucasian defendants. In other words, more African-American were labeled high risk and ended up not relapsing into criminal behaviour than Caucasian defendants. A more thorough explanation of their data analysis procedure can be found in their companion article [How We Analyzed the COMPAS Recidivism Algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm).\n",
    "\n",
    "The COMPAS algorithm is proprietary software. What is known is that its decision is based on the answers to a questionnaire with 137 questions which the defendant must fill. There are questions related to crime (“How many prior juvenile felony offense arrests?”) along with seemingly mundane ones (“Do you live with friends?”; “Do you feel discouraged at times?”).\n",
    "\n",
    "In this hackathon, you are provided a dataset with a subset of answers to that questionnaire from more than 7000 defendants living in Broward County, Florida as well as whether they did relapse into criminal behaviour or not.\n",
    "\n",
    "## Objective(s)\n",
    "It has been shown in the article linked above that the COMPAS algorithm is biased. We take this for granted and we do not bother to show it again. The main objective of the hackathon for you is to understand that it is not only the COMPAS algorithm that is biased, but that **the data itself is biased**, in the sense that one can find structural patterns of discrimination embedded in the data. In other words, *learning from data coming from a biased world without precautions will necessarily lead to biased predictions*. Knowing which precautions one should take to avoid biased predictions is a whole subfield of machine learning called \"Fairness in AI\". It is out of the scope of this hackathon and out of the scope of LEPL1109. If you are curious about it however, a great resource is the following book [Fairness and machine learning Limitations and Opportunities](https://fairmlbook.org/).\n",
    "\n",
    "To see that the data itself is biased, you will implement your own recidivism prediction algorithms and measure their fairness.\n",
    "\n",
    "## Dataset description\n",
    "A large part of this hackathon will be devoted to handling, understanding and manipulating the dataset. The dataset provided represents 7214 defendants (data points) with 49 answers to the questionnaire (features) for each defendant. This is a lot of features and it should take you some time to understand them before going through Part 1. This data processing part may be tiresome but it is a necessary task in any serious data project.\n",
    "A description of the features is provided in the table below:\n",
    "\n",
    "| Feature               | Description                                                                                      |\n",
    "|-----------------------|--------------------------------------------------------------------------------------------------|\n",
    "| name                  | Full name of the defendant                                                                       |\n",
    "| first                 | First name of the defendant                                                                      |\n",
    "| last                  | Last name of the defendant                                                                       |\n",
    "| compas_screening_date | The date the defendant filled the questionnaire                                                  |\n",
    "| sex                   | Sex of the defendant (Female, Male)                                                              |\n",
    "| dob                   | Date of birth of the defendant (YYYY-MM-DD)                                                      |\n",
    "| age                   | Age of the defendant                                                                             |\n",
    "| age_cat               | Age category of the defendant (Less than 25, 25-45, Greater than 45)                             |\n",
    "| race                  | race attribute (African-American, Caucasian, Hispanic, Asian, Native American, Other)       |\n",
    "| juv_fel_count         | Number of juvenile felonies committed by the defendant                                           |\n",
    "| decile_score          | Decile of the COMPAS score                                                                       |\n",
    "| juv_misd_count        | Number of juvenile misdemeanors                                                                  |\n",
    "| juv_other_count       | Number of juvenile convictions that are not considered misdemeanors nor felonies                 |\n",
    "| priors_count          | Number of prior crimes committed                                                                 |\n",
    "| days_b_screening_arrest | Count of days between screening date and (original) arrest date                                |\n",
    "| c_jail_in             | Datetime at which the defendant entered jail (YYYY-MM-DD, hh:mm:ss)                              |\n",
    "| c_jail_out            | Datetime at which the defendant left jail (YYYY-MM-DD, hh:mm:ss)                                 |\n",
    "| c_case_number         | Case number for the current charge                                                               |\n",
    "| c_offense_date        | Date the offense was committed (YYYY-MM-DD)                                                      |\n",
    "| c_arrest_date         | Date the offense was arrested (YYYY-MM-DD)                                                       |\n",
    "| c_days_from_compas    | Days from COMPAS screening date to current arrest date                                           |\n",
    "| c_charge_degree       | Current charge degree (felony or misdemeanor) at the time of filling the questionnaire (\"F\", \"M\")|\n",
    "| c_charge_desc         | Description of the current charge                                                                |\n",
    "| is_recid              | Binary variable indicating whether the defendant is rearrested at any time (0, 1)                |\n",
    "| r_case_number         | Case number for a recidivism charge                                                              |\n",
    "| r_charge_degree       | Recidivism charge degree (felony or misdemeanor) for an offense subsequent to filling the questionnaire |\n",
    "| r_days_from_arrest    | Days from Arrest to Recidivism Event                                                             |\n",
    "| r_offense_date        | Date the recidivism offense was committed (YYYY-MM-DD)                                           |\n",
    "| r_charge_desc         | Description of the recidivism charge                                                             |\n",
    "| r_jail_in             | Datetime at which the defendant entered jail for a recidivism charge (YYYY-MM-DD, hh:mm:ss)      |\n",
    "| r_jail_out            | Datetime at which the defendant left jail for a recidivism charge (YYYY-MM-DD, hh:mm:ss)         |\n",
    "| violent_recid         | Number of violent recidivism events                                                              |\n",
    "| is_violent_recid      | Binary variable indicating whether the defendant committed a violent recidivism (0, 1)           |\n",
    "| vr_case_number        | Case number for a violent recidivism charge                                                      |\n",
    "| vr_charge_degree      | Violent recidivism charge degree (felony or misdemeanor)                                         |\n",
    "| vr_offense_date       | Date the violent recidivism offense was committed (YYYY-MM-DD)                                   |\n",
    "| vr_charge_desc        | Description of the violent recidivism charge                                                     |\n",
    "| type_of_assessment    | Type of COMPAS assessment performed                                                              |\n",
    "| decile_score.1        | *Same as decile_score*                                                                           |\n",
    "| score_text            | Recidivism risk of the defendant (Low, Medium, High)                                             |\n",
    "| screening_date        | Date on which the defendant was screened (YYYY-MM-DD)                                            |\n",
    "| v_type_of_assessment  | Type of violent risk assessment                                                                  |\n",
    "| v_decile_score        | Decile score for violent risk assessment                                                         |\n",
    "| v_score_text          | Violent recidivism risk of the defendant (Low, Medium, High)                                     |\n",
    "| v_screening_date      | Date of the violent risk assessment (YYYY-MM-DD)                                                 |\n",
    "| in_custody            | Date on which the defendant was placed in custody (YYYY-MM-DD)                                   |\n",
    "| out_custody           | Date on which the defendant left custody (YYYY-MM-DD)                                            |\n",
    "| priors_count.1        | *Same as priors_count*                                                                           |\n",
    "| two_year_recid        | Binary variable on whether the defendant has recidivated within two years (0, 1)                 |\n",
    "\n",
    "## **Notebook structure**\n",
    "\n",
    "### PART 1 - Data preprocessing\n",
    "   #### 1.1 - Importing the packages\n",
    "   #### 1.2 - Importing the dataset\n",
    "   #### 1.3 - Dataset curation\n",
    "   #### 1.4 - Feature engineering\n",
    "   #### 1.5 - Sensitive features\n",
    "   #### 1.6 - Scale the dataset\n",
    "\n",
    "### PART 2 - Data exploration\n",
    "   #### 2.1 - Feature visualization\n",
    "   #### 2.2 - Principal Component Analysis\n",
    "\n",
    "   \n",
    "### PART 3 - Clustering\n",
    "   #### 3.1 - K-Means\n",
    "   #### 3.2 - Results Analysis\n",
    "\n",
    "\n",
    "### PART 4 - Validation and fairness metrics\n",
    "   #### 4.1 - Silhouette score\n",
    "   #### 4.2 - Purity and entropy of a clustering\n",
    "   #### 4.3 - Precision and Recall per race group\n",
    "   #### 4.4 - Select the number of clusters\n",
    "\n",
    "\n",
    "### PART 5 - Visualization\n",
    "   #### 5.1 - Visualize your results\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***Remark***\n",
    "\n",
    "We filled this notebook with preliminary (trivial) code. This practice makes possible to run each cell, even the last ones, without throwing warnings once the dataset is imported. <b>Take advantage of this aspect to divide the work between all team members!</b> <br><br>\n",
    "Remember that many libraries exist in Python, so many functions have already been developed. Read the documentation and don't reinvent the wheel! You can import whatever you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART I - Preliminaries</b> </font> <br><br>\n",
    "\n",
    "<font size=5 color=#009999> <b>1.1 - Importing the packages</b> <br>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1.1 : IMPORTING ALL THE NECESSARY PACKAGES\n",
    "\n",
    "@pre:  /\n",
    "@post: The necessary packages should be loaded.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all the necessary packages here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.2 - Importing the dataset</b> <br>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7214 entries, 0 to 7213\n",
      "Data columns (total 49 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   name                     7214 non-null   object \n",
      " 1   first                    7214 non-null   object \n",
      " 2   last                     7214 non-null   object \n",
      " 3   compas_screening_date    7214 non-null   object \n",
      " 4   sex                      7214 non-null   object \n",
      " 5   dob                      7214 non-null   object \n",
      " 6   age                      7214 non-null   int64  \n",
      " 7   age_cat                  7214 non-null   object \n",
      " 8   race                     7214 non-null   object \n",
      " 9   juv_fel_count            7214 non-null   int64  \n",
      " 10  decile_score             7214 non-null   int64  \n",
      " 11  juv_misd_count           7214 non-null   int64  \n",
      " 12  juv_other_count          7214 non-null   int64  \n",
      " 13  priors_count             7214 non-null   int64  \n",
      " 14  days_b_screening_arrest  6907 non-null   float64\n",
      " 15  c_jail_in                6907 non-null   object \n",
      " 16  c_jail_out               6907 non-null   object \n",
      " 17  c_case_number            7192 non-null   object \n",
      " 18  c_offense_date           6055 non-null   object \n",
      " 19  c_arrest_date            1137 non-null   object \n",
      " 20  c_days_from_compas       7192 non-null   float64\n",
      " 21  c_charge_degree          7214 non-null   object \n",
      " 22  c_charge_desc            7185 non-null   object \n",
      " 23  is_recid                 7214 non-null   int64  \n",
      " 24  r_case_number            3471 non-null   object \n",
      " 25  r_charge_degree          3471 non-null   object \n",
      " 26  r_days_from_arrest       2316 non-null   float64\n",
      " 27  r_offense_date           3471 non-null   object \n",
      " 28  r_charge_desc            3413 non-null   object \n",
      " 29  r_jail_in                2316 non-null   object \n",
      " 30  r_jail_out               2316 non-null   object \n",
      " 31  violent_recid            0 non-null      float64\n",
      " 32  is_violent_recid         7214 non-null   int64  \n",
      " 33  vr_case_number           819 non-null    object \n",
      " 34  vr_charge_degree         819 non-null    object \n",
      " 35  vr_offense_date          819 non-null    object \n",
      " 36  vr_charge_desc           819 non-null    object \n",
      " 37  type_of_assessment       7214 non-null   object \n",
      " 38  decile_score.1           7214 non-null   int64  \n",
      " 39  score_text               7214 non-null   object \n",
      " 40  screening_date           7214 non-null   object \n",
      " 41  v_type_of_assessment     7214 non-null   object \n",
      " 42  v_decile_score           7214 non-null   int64  \n",
      " 43  v_score_text             7214 non-null   object \n",
      " 44  v_screening_date         7214 non-null   object \n",
      " 45  in_custody               6978 non-null   object \n",
      " 46  out_custody              6978 non-null   object \n",
      " 47  priors_count.1           7214 non-null   int64  \n",
      " 48  two_year_recid           7214 non-null   int64  \n",
      "dtypes: float64(4), int64(12), object(33)\n",
      "memory usage: 2.7+ MB\n",
      "                 name   first         last compas_screening_date   sex  \\\n",
      "0    miguel hernandez  miguel    hernandez            2013-08-14  Male   \n",
      "1         kevon dixon   kevon        dixon            2013-01-27  Male   \n",
      "2            ed philo      ed        philo            2013-04-14  Male   \n",
      "3         marcu brown   marcu        brown            2013-01-13  Male   \n",
      "4  bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male   \n",
      "\n",
      "          dob  age          age_cat              race  juv_fel_count  ...  \\\n",
      "0  1947-04-18   69  Greater than 45             Other              0  ...   \n",
      "1  1982-01-22   34          25 - 45  African-American              0  ...   \n",
      "2  1991-05-14   24     Less than 25  African-American              0  ...   \n",
      "3  1993-01-21   23     Less than 25  African-American              0  ...   \n",
      "4  1973-01-22   43          25 - 45             Other              0  ...   \n",
      "\n",
      "   score_text  screening_date  v_type_of_assessment  v_decile_score  \\\n",
      "0         Low      2013-08-14      Risk of Violence               1   \n",
      "1         Low      2013-01-27      Risk of Violence               1   \n",
      "2         Low      2013-04-14      Risk of Violence               3   \n",
      "3        High      2013-01-13      Risk of Violence               6   \n",
      "4         Low      2013-03-26      Risk of Violence               1   \n",
      "\n",
      "   v_score_text v_screening_date  in_custody out_custody priors_count.1  \\\n",
      "0           Low       2013-08-14  2014-07-07  2014-07-14              0   \n",
      "1           Low       2013-01-27  2013-01-26  2013-02-05              0   \n",
      "2           Low       2013-04-14  2013-06-16  2013-06-16              4   \n",
      "3        Medium       2013-01-13         NaN         NaN              1   \n",
      "4           Low       2013-03-26         NaN         NaN              2   \n",
      "\n",
      "  two_year_recid  \n",
      "0              0  \n",
      "1              1  \n",
      "2              1  \n",
      "3              0  \n",
      "4              0  \n",
      "\n",
      "[5 rows x 49 columns]\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# CELL N°1.2 : IMPORTING THE DATASET\n",
    "\n",
    "# @pre:  /\n",
    "# @post: The object df should contain a Pandas DataFrame corresponding to the file compas-dataset.csv.\n",
    "# \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Importer le fichier CSV\n",
    "file_path = 'compas-dataset.csv'  # Remplacez par le chemin exact si nécessaire\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Afficher les informations de base sur le dataset\n",
    "df.info()\n",
    "\n",
    "# Aperçu des premières lignes\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.3 - Dataset curation</b> <br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this hackathon, your goal is to **determine the risk of recidivism of an individual**. Therefore, you should be able to determine which features are useful for your application and remove the unnecessary ones. We provide a list of features to keep and ask you to add features to that list. This step may take more time than the others. It is important to carefully analyze each feature and its relevance for our goal.\n",
    "\n",
    "In this data cleaning task, you must remove redundant features, features that are not quantifiable and features that you believe are not linked to risk of recidivism. Yous should neither limit yourself to the provided list which is too short nor add all numeric features.\n",
    "You should also avoid data leakage. Except for the \"two_year_recid\" feature, do not keep features which represent true recidivism. For similar reasons, do not keep features linked to the predictions made by the COMPAS algorithm. Using predictions made by a supervised algorithm (which is trained using both the features matrix and the target variable) is effectively leaking information from the target variable.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.1] Removing unnecessary features </b>  <br>\n",
    "Can you already, a priori, detect that some features are useless?\n",
    "<ol>\n",
    "   <li> if yes, list those (useless) features and explain your choice;\n",
    "   <li> if not, then explain why it is better to wait.\n",
    "</ol>\n",
    "    Generally speaking, is it a good idea to remove a feature based on <i>a priori</i> knowledge, or doesn't it alter the final outcome?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_jail_in</th>\n",
       "      <th>c_jail_out</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>...</th>\n",
       "      <th>two_year_recid</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <th>c_offense_date</th>\n",
       "      <th>c_arrest_date</th>\n",
       "      <th>c_days_from_compas</th>\n",
       "      <th>type_of_assessment</th>\n",
       "      <th>v_type_of_assessment</th>\n",
       "      <th>screening_date</th>\n",
       "      <th>v_screening_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6172</td>\n",
       "      <td>6172.000000</td>\n",
       "      <td>6172</td>\n",
       "      <td>6172.000000</td>\n",
       "      <td>6172.000000</td>\n",
       "      <td>6172.000000</td>\n",
       "      <td>6172.000000</td>\n",
       "      <td>6172</td>\n",
       "      <td>6172</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>6172.000000</td>\n",
       "      <td>6172</td>\n",
       "      <td>6172.000000</td>\n",
       "      <td>5388</td>\n",
       "      <td>784</td>\n",
       "      <td>6172.000000</td>\n",
       "      <td>6172</td>\n",
       "      <td>6172</td>\n",
       "      <td>6172</td>\n",
       "      <td>6172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>702</td>\n",
       "      <td>760</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>736</td>\n",
       "      <td>416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>685</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>African-American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-02-22</td>\n",
       "      <td>2013-02-15</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-04-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>2013-02-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>2013-04-20</td>\n",
       "      <td>2013-04-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>3970</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6172</td>\n",
       "      <td>6172</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34.534511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.091218</td>\n",
       "      <td>0.110661</td>\n",
       "      <td>3.246436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.740279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.903273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11.730938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.463599</td>\n",
       "      <td>0.497872</td>\n",
       "      <td>0.470731</td>\n",
       "      <td>4.743770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.084709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>276.812982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-30.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9485.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sex          age              race  juv_fel_count  juv_misd_count  \\\n",
       "count   6172  6172.000000              6172    6172.000000     6172.000000   \n",
       "unique     2          NaN                 6            NaN             NaN   \n",
       "top     Male          NaN  African-American            NaN             NaN   \n",
       "freq    4997          NaN              3175            NaN             NaN   \n",
       "mean     NaN    34.534511               NaN       0.059300        0.091218   \n",
       "std      NaN    11.730938               NaN       0.463599        0.497872   \n",
       "min      NaN    18.000000               NaN       0.000000        0.000000   \n",
       "25%      NaN    25.000000               NaN       0.000000        0.000000   \n",
       "50%      NaN    31.000000               NaN       0.000000        0.000000   \n",
       "75%      NaN    42.000000               NaN       0.000000        0.000000   \n",
       "max      NaN    96.000000               NaN      20.000000       13.000000   \n",
       "\n",
       "        juv_other_count  priors_count   c_jail_in  c_jail_out c_charge_degree  \\\n",
       "count       6172.000000   6172.000000        6172        6172            6172   \n",
       "unique              NaN           NaN         702         760               2   \n",
       "top                 NaN           NaN  2013-02-22  2013-02-15               F   \n",
       "freq                NaN           NaN          31          29            3970   \n",
       "mean           0.110661      3.246436         NaN         NaN             NaN   \n",
       "std            0.470731      4.743770         NaN         NaN             NaN   \n",
       "min            0.000000      0.000000         NaN         NaN             NaN   \n",
       "25%            0.000000      0.000000         NaN         NaN             NaN   \n",
       "50%            0.000000      1.000000         NaN         NaN             NaN   \n",
       "75%            0.000000      4.000000         NaN         NaN             NaN   \n",
       "max            9.000000     38.000000         NaN         NaN             NaN   \n",
       "\n",
       "        ... two_year_recid compas_screening_date  days_b_screening_arrest  \\\n",
       "count   ...    6172.000000                  6172              6172.000000   \n",
       "unique  ...            NaN                   685                      NaN   \n",
       "top     ...            NaN            2013-04-20                      NaN   \n",
       "freq    ...            NaN                    30                      NaN   \n",
       "mean    ...       0.455120                   NaN                -1.740279   \n",
       "std     ...       0.498022                   NaN                 5.084709   \n",
       "min     ...       0.000000                   NaN               -30.000000   \n",
       "25%     ...       0.000000                   NaN                -1.000000   \n",
       "50%     ...       0.000000                   NaN                -1.000000   \n",
       "75%     ...       1.000000                   NaN                -1.000000   \n",
       "max     ...       1.000000                   NaN                30.000000   \n",
       "\n",
       "       c_offense_date  c_arrest_date c_days_from_compas  type_of_assessment  \\\n",
       "count            5388            784        6172.000000                6172   \n",
       "unique            736            416                NaN                   1   \n",
       "top        2013-01-14     2013-02-06                NaN  Risk of Recidivism   \n",
       "freq               24              8                NaN                6172   \n",
       "mean              NaN            NaN          24.903273                 NaN   \n",
       "std               NaN            NaN         276.812982                 NaN   \n",
       "min               NaN            NaN           0.000000                 NaN   \n",
       "25%               NaN            NaN           1.000000                 NaN   \n",
       "50%               NaN            NaN           1.000000                 NaN   \n",
       "75%               NaN            NaN           1.000000                 NaN   \n",
       "max               NaN            NaN        9485.000000                 NaN   \n",
       "\n",
       "        v_type_of_assessment screening_date v_screening_date  \n",
       "count                   6172           6172             6172  \n",
       "unique                     1            685              685  \n",
       "top         Risk of Violence     2013-04-20       2013-04-20  \n",
       "freq                    6172             30               30  \n",
       "mean                     NaN            NaN              NaN  \n",
       "std                      NaN            NaN              NaN  \n",
       "min                      NaN            NaN              NaN  \n",
       "25%                      NaN            NaN              NaN  \n",
       "50%                      NaN            NaN              NaN  \n",
       "75%                      NaN            NaN              NaN  \n",
       "max                      NaN            NaN              NaN  \n",
       "\n",
       "[11 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL N°1.3.1 : CURATION OF THE DATASET\n",
    "\n",
    "@pre:  A pandas.DataFrame df containing the dataset\n",
    "@post: A pandas.DataFrame df containing the dataset without outliers and with necessary features only\n",
    "\"\"\"\n",
    "\n",
    "# Suppression des lignes correspondant à des cas particuliers ou des données de mauvaise qualité\n",
    "df = df[\n",
    "    (\n",
    "        df[\"is_recid\"] != -1\n",
    "    )  # Data aggregator encoded is_recid = -1 whenever they couldn't find a COMPAS case.\n",
    "    & (\n",
    "        df[\"days_b_screening_arrest\"] <= 30\n",
    "    )  # More than 30 days between the day of arrest and the date when the questionnaire was filled => poor data quality\n",
    "    & (df[\"days_b_screening_arrest\"] >= -30)  # Same as above\n",
    "    & (\n",
    "        df[\"c_charge_degree\"] != \"O\"\n",
    "    )  # These are simple traffic offenses, they will never be charged with jail.\n",
    "]\n",
    "\n",
    "# Liste des colonnes à conserver\n",
    "columns_to_keep = [\n",
    "    \"sex\",\n",
    "    \"age\",\n",
    "    \"race\",\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "    \"priors_count\",\n",
    "    \"c_jail_in\",\n",
    "    \"c_jail_out\",\n",
    "    \"c_charge_degree\",\n",
    "    \"in_custody\",\n",
    "    \"out_custody\",\n",
    "    \"two_year_recid\",\n",
    "    \"compas_screening_date\", \n",
    "    \"days_b_screening_arrest\", \n",
    "    \"c_offense_date\", \n",
    "    \"c_arrest_date\",\n",
    "    \"c_days_from_compas\", \n",
    "    \"type_of_assessment\",\n",
    "    \"v_type_of_assessment\", \n",
    "    \"screening_date\",\n",
    "    \"v_screening_date\"\n",
    "]\n",
    "\n",
    "# Création d'un DataFrame ne contenant que ces colonnes\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Vérification de la structure du DataFrame après la sélection\n",
    "df.describe(include=\"all\")\n",
    "\n",
    "# Remarque : on aimerait retirer, mais pas surs de comment justifier a ce stade :\n",
    "# 'days_b_screening_arrest', 'c_days_from_compas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data science, datasets are rarely tailored to specific applications. Instead, they typically originate from information collected over a certain period. It is the data scientist's responsibility to effectively utilize these datasets.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark 1.1]</b><br>\n",
    "In most real-world cases, the datasets you work with will contain artifacts like typos or missing data, which may need to be removed before using them in algorithms. In Pandas, missing data is represented as \"NaNs\" (Not a Number), though it applies to all missing objects, not just numbers.\n",
    "</div>\n",
    "\n",
    "Can you find a way to inspect your dataset and see if there are some missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Missing Values  Percentage\n",
      "sex                                   0    0.000000\n",
      "age                                   0    0.000000\n",
      "race                                  0    0.000000\n",
      "juv_fel_count                         0    0.000000\n",
      "juv_misd_count                        0    0.000000\n",
      "juv_other_count                       0    0.000000\n",
      "priors_count                          0    0.000000\n",
      "c_jail_in                             0    0.000000\n",
      "c_jail_out                            0    0.000000\n",
      "c_charge_degree                       0    0.000000\n",
      "in_custody                            0    0.000000\n",
      "out_custody                           0    0.000000\n",
      "two_year_recid                        0    0.000000\n",
      "compas_screening_date                 0    0.000000\n",
      "days_b_screening_arrest               0    0.000000\n",
      "c_offense_date                      784   12.702528\n",
      "c_arrest_date                      5388   87.297472\n",
      "c_days_from_compas                    0    0.000000\n",
      "type_of_assessment                    0    0.000000\n",
      "v_type_of_assessment                  0    0.000000\n",
      "screening_date                        0    0.000000\n",
      "v_screening_date                      0    0.000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL N°1.3.2: INFORMATION ABOUT TYPES AND NANs\n",
    "@pre:  A pandas.DataFrame df containing the dataset.\n",
    "@post: Statistics and/or visualization on the presence of missing data in df.\n",
    "\"\"\"\n",
    "# Calcul du nombre de valeurs manquantes et du pourcentage de NaNs par colonne\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().mean()) * 100\n",
    "\n",
    "# Création d'un tableau combinant le nombre de NaNs et le pourcentage de NaNs\n",
    "missing_info = pd.DataFrame({\n",
    "    'Missing Values': missing_data,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "\n",
    "# Affichage du tableau\n",
    "print(missing_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark 1.2] Each problem has its own solution</b> <br>\n",
    "There exist numerous ways to deal with missing information and we will discuss the two main approaches:\n",
    "<ol>\n",
    "   <li> you remove rows or columns that contain missing data;\n",
    "   <li> or you replace NaNs with another value. The latter can be a fixed value or computed to be, e.g., the mean of all non-NaNs values. The topic of replacing missing data, also called imputation of missing values, is very broad and complex, and there is no global solution that applies everywhere. Maybe you can find one that works well here?\n",
    "</ol>\n",
    "    \n",
    "You **should** read more about how to imput missing value [here](https://scikit-learn.org/stable/modules/impute.html). However, you will not be evaluated on how sophisticated your handling of NaNs is so, for this hackathon, do not spend an unreasonable amount of time on the next cell.\n",
    "</div> \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.2] Handling missing data </b>  <br>\n",
    "Given the dataset and the amount / type of missing information, what strategy do you propose to follow regarding missing data (NaNs)? <br> You can choose one or many of the following:\n",
    "<ol>\n",
    "   <li> drop features (column) with missing information; \n",
    "   <li> drop samples (row) with missing information;\n",
    "   <li> replace missing information with interpolation / extrapolation / simple substitution / ...\n",
    "</ol>\n",
    "Justify briefly your choice.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5388 entries, 0 to 7213\n",
      "Data columns (total 21 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   sex                      5388 non-null   object \n",
      " 1   age                      5388 non-null   int64  \n",
      " 2   race                     5388 non-null   object \n",
      " 3   juv_fel_count            5388 non-null   int64  \n",
      " 4   juv_misd_count           5388 non-null   int64  \n",
      " 5   juv_other_count          5388 non-null   int64  \n",
      " 6   priors_count             5388 non-null   int64  \n",
      " 7   c_jail_in                5388 non-null   object \n",
      " 8   c_jail_out               5388 non-null   object \n",
      " 9   c_charge_degree          5388 non-null   object \n",
      " 10  in_custody               5388 non-null   object \n",
      " 11  out_custody              5388 non-null   object \n",
      " 12  two_year_recid           5388 non-null   int64  \n",
      " 13  compas_screening_date    5388 non-null   object \n",
      " 14  days_b_screening_arrest  5388 non-null   float64\n",
      " 15  c_offense_date           5388 non-null   object \n",
      " 16  c_days_from_compas       5388 non-null   float64\n",
      " 17  type_of_assessment       5388 non-null   object \n",
      " 18  v_type_of_assessment     5388 non-null   object \n",
      " 19  screening_date           5388 non-null   object \n",
      " 20  v_screening_date         5388 non-null   object \n",
      "dtypes: float64(2), int64(6), object(13)\n",
      "memory usage: 926.1+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5388 entries, 0 to 7213\n",
      "Data columns (total 21 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   sex                      5388 non-null   object \n",
      " 1   age                      5388 non-null   int64  \n",
      " 2   race                     5388 non-null   object \n",
      " 3   juv_fel_count            5388 non-null   int64  \n",
      " 4   juv_misd_count           5388 non-null   int64  \n",
      " 5   juv_other_count          5388 non-null   int64  \n",
      " 6   priors_count             5388 non-null   int64  \n",
      " 7   c_jail_in                5388 non-null   object \n",
      " 8   c_jail_out               5388 non-null   object \n",
      " 9   c_charge_degree          5388 non-null   object \n",
      " 10  in_custody               5388 non-null   object \n",
      " 11  out_custody              5388 non-null   object \n",
      " 12  two_year_recid           5388 non-null   int64  \n",
      " 13  compas_screening_date    5388 non-null   object \n",
      " 14  days_b_screening_arrest  5388 non-null   float64\n",
      " 15  c_offense_date           5388 non-null   object \n",
      " 16  c_days_from_compas       5388 non-null   float64\n",
      " 17  type_of_assessment       5388 non-null   object \n",
      " 18  v_type_of_assessment     5388 non-null   object \n",
      " 19  screening_date           5388 non-null   object \n",
      " 20  v_screening_date         5388 non-null   object \n",
      "dtypes: float64(2), int64(6), object(13)\n",
      "memory usage: 926.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_jail_in</th>\n",
       "      <th>c_jail_out</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>...</th>\n",
       "      <th>out_custody</th>\n",
       "      <th>two_year_recid</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <th>c_offense_date</th>\n",
       "      <th>c_days_from_compas</th>\n",
       "      <th>type_of_assessment</th>\n",
       "      <th>v_type_of_assessment</th>\n",
       "      <th>screening_date</th>\n",
       "      <th>v_screening_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5388</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388</td>\n",
       "      <td>...</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>687</td>\n",
       "      <td>746</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>678</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>African-American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-02-22</td>\n",
       "      <td>2013-04-30</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-04-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>2013-04-20</td>\n",
       "      <td>2013-04-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>3299</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34.531366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.049183</td>\n",
       "      <td>0.085746</td>\n",
       "      <td>0.112101</td>\n",
       "      <td>3.048998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.455642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.612843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.465293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11.764565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.373119</td>\n",
       "      <td>0.482934</td>\n",
       "      <td>0.474775</td>\n",
       "      <td>4.664760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.498075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.716708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229.852096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-30.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9485.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sex          age              race  juv_fel_count  juv_misd_count  \\\n",
       "count   5388  5388.000000              5388    5388.000000     5388.000000   \n",
       "unique     2          NaN                 6            NaN             NaN   \n",
       "top     Male          NaN  African-American            NaN             NaN   \n",
       "freq    4351          NaN              2717            NaN             NaN   \n",
       "mean     NaN    34.531366               NaN       0.049183        0.085746   \n",
       "std      NaN    11.764565               NaN       0.373119        0.482934   \n",
       "min      NaN    19.000000               NaN       0.000000        0.000000   \n",
       "25%      NaN    25.000000               NaN       0.000000        0.000000   \n",
       "50%      NaN    31.000000               NaN       0.000000        0.000000   \n",
       "75%      NaN    42.000000               NaN       0.000000        0.000000   \n",
       "max      NaN    96.000000               NaN      10.000000       13.000000   \n",
       "\n",
       "        juv_other_count  priors_count   c_jail_in  c_jail_out c_charge_degree  \\\n",
       "count       5388.000000   5388.000000        5388        5388            5388   \n",
       "unique              NaN           NaN         687         746               2   \n",
       "top                 NaN           NaN  2013-02-22  2013-04-30               F   \n",
       "freq                NaN           NaN          25          24            3299   \n",
       "mean           0.112101      3.048998         NaN         NaN             NaN   \n",
       "std            0.474775      4.664760         NaN         NaN             NaN   \n",
       "min            0.000000      0.000000         NaN         NaN             NaN   \n",
       "25%            0.000000      0.000000         NaN         NaN             NaN   \n",
       "50%            0.000000      1.000000         NaN         NaN             NaN   \n",
       "75%            0.000000      4.000000         NaN         NaN             NaN   \n",
       "max            9.000000     38.000000         NaN         NaN             NaN   \n",
       "\n",
       "        ... out_custody two_year_recid  compas_screening_date  \\\n",
       "count   ...        5388    5388.000000                   5388   \n",
       "unique  ...        1070            NaN                    678   \n",
       "top     ...  2020-01-01            NaN             2013-04-20   \n",
       "freq    ...          35            NaN                     25   \n",
       "mean    ...         NaN       0.455642                    NaN   \n",
       "std     ...         NaN       0.498075                    NaN   \n",
       "min     ...         NaN       0.000000                    NaN   \n",
       "25%     ...         NaN       0.000000                    NaN   \n",
       "50%     ...         NaN       0.000000                    NaN   \n",
       "75%     ...         NaN       1.000000                    NaN   \n",
       "max     ...         NaN       1.000000                    NaN   \n",
       "\n",
       "       days_b_screening_arrest  c_offense_date c_days_from_compas  \\\n",
       "count              5388.000000            5388        5388.000000   \n",
       "unique                     NaN             736                NaN   \n",
       "top                        NaN      2013-01-14                NaN   \n",
       "freq                       NaN              24                NaN   \n",
       "mean                 -1.612843             NaN          17.465293   \n",
       "std                   4.716708             NaN         229.852096   \n",
       "min                 -30.000000             NaN           0.000000   \n",
       "25%                  -1.000000             NaN           1.000000   \n",
       "50%                  -1.000000             NaN           1.000000   \n",
       "75%                  -1.000000             NaN           1.000000   \n",
       "max                  30.000000             NaN        9485.000000   \n",
       "\n",
       "        type_of_assessment v_type_of_assessment screening_date  \\\n",
       "count                 5388                 5388           5388   \n",
       "unique                   1                    1            678   \n",
       "top     Risk of Recidivism     Risk of Violence     2013-04-20   \n",
       "freq                  5388                 5388             25   \n",
       "mean                   NaN                  NaN            NaN   \n",
       "std                    NaN                  NaN            NaN   \n",
       "min                    NaN                  NaN            NaN   \n",
       "25%                    NaN                  NaN            NaN   \n",
       "50%                    NaN                  NaN            NaN   \n",
       "75%                    NaN                  NaN            NaN   \n",
       "max                    NaN                  NaN            NaN   \n",
       "\n",
       "       v_screening_date  \n",
       "count              5388  \n",
       "unique              678  \n",
       "top          2013-04-20  \n",
       "freq                 25  \n",
       "mean                NaN  \n",
       "std                 NaN  \n",
       "min                 NaN  \n",
       "25%                 NaN  \n",
       "50%                 NaN  \n",
       "75%                 NaN  \n",
       "max                 NaN  \n",
       "\n",
       "[11 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL N°1.3.3: Handling missing values\n",
    "@pre:  A pandas.DataFrame df containing the dataset.\n",
    "@post: A pandas.DataFrame df containing the dataset with no missing values.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "threshold = 0.8  # 80%\n",
    "na_percentage = df.isna().mean()\n",
    "\n",
    "# Filtrer les colonnes qui ont moins de 80% de NaN\n",
    "columns_to_keep = na_percentage[na_percentage < threshold].index\n",
    "\n",
    "# Suppression des lignes contenant des NaNs\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Vérification après suppression\n",
    "print(df.info())\n",
    "\n",
    "df.info()\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.4 - Feature engineering</b> <br>\n",
    "</font>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark 1.3] New features extraction</b> <br>\n",
    "In the present case, some features in the dataset still need to be reworked in order to provide meaningful information. For example, working with datetimes might not be easy.\n",
    "</div>\n",
    "\n",
    "You may want to somehow incorporate the information about date and time into the dataset in a more **intelligent** manner than it was before. Again, there can be multiple solutions, and we will propose you a very simple one.\n",
    "\n",
    "For example, what is most important to predict the likelihood of recidivism: the exact dates at which each defendant entered and left jail/custody or the time spent in jail/custody?\n",
    "\n",
    "Note: 1) you should apply your solution to all the \"date/time\" features you kept. There should at least be the one hinted above. 2) Pandas has a to_datetime function that should prove useful !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5388 entries, 0 to 7213\n",
      "Data columns (total 19 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   sex                      5388 non-null   object \n",
      " 1   age                      5388 non-null   int64  \n",
      " 2   race                     5388 non-null   object \n",
      " 3   juv_fel_count            5388 non-null   int64  \n",
      " 4   juv_misd_count           5388 non-null   int64  \n",
      " 5   juv_other_count          5388 non-null   int64  \n",
      " 6   priors_count             5388 non-null   int64  \n",
      " 7   c_charge_degree          5388 non-null   int64  \n",
      " 8   two_year_recid           5388 non-null   int64  \n",
      " 9   compas_screening_date    5388 non-null   object \n",
      " 10  days_b_screening_arrest  5388 non-null   float64\n",
      " 11  c_offense_date           5388 non-null   object \n",
      " 12  c_days_from_compas       5388 non-null   float64\n",
      " 13  type_of_assessment       5388 non-null   int64  \n",
      " 14  v_type_of_assessment     5388 non-null   int64  \n",
      " 15  screening_date           5388 non-null   object \n",
      " 16  v_screening_date         5388 non-null   object \n",
      " 17  days_in_jail             5388 non-null   int32  \n",
      " 18  days_in_custody          5388 non-null   int32  \n",
      "dtypes: float64(2), int32(2), int64(9), object(6)\n",
      "memory usage: 799.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>two_year_recid</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <th>c_offense_date</th>\n",
       "      <th>c_days_from_compas</th>\n",
       "      <th>type_of_assessment</th>\n",
       "      <th>v_type_of_assessment</th>\n",
       "      <th>screening_date</th>\n",
       "      <th>v_screening_date</th>\n",
       "      <th>days_in_jail</th>\n",
       "      <th>days_in_custody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5388</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388.0</td>\n",
       "      <td>5388.0</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388</td>\n",
       "      <td>5388.000000</td>\n",
       "      <td>5388.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>678</td>\n",
       "      <td>678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>African-American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-04-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-04-20</td>\n",
       "      <td>2013-04-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34.531366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.049183</td>\n",
       "      <td>0.085746</td>\n",
       "      <td>0.112101</td>\n",
       "      <td>3.048998</td>\n",
       "      <td>0.612287</td>\n",
       "      <td>0.455642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.612843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.465293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.452858</td>\n",
       "      <td>33.182442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11.764565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.373119</td>\n",
       "      <td>0.482934</td>\n",
       "      <td>0.474775</td>\n",
       "      <td>4.664760</td>\n",
       "      <td>0.487274</td>\n",
       "      <td>0.498075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.716708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229.852096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.930678</td>\n",
       "      <td>169.105520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-30.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9485.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>749.000000</td>\n",
       "      <td>6035.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sex          age              race  juv_fel_count  juv_misd_count  \\\n",
       "count   5388  5388.000000              5388    5388.000000     5388.000000   \n",
       "unique     2          NaN                 6            NaN             NaN   \n",
       "top     Male          NaN  African-American            NaN             NaN   \n",
       "freq    4351          NaN              2717            NaN             NaN   \n",
       "mean     NaN    34.531366               NaN       0.049183        0.085746   \n",
       "std      NaN    11.764565               NaN       0.373119        0.482934   \n",
       "min      NaN    19.000000               NaN       0.000000        0.000000   \n",
       "25%      NaN    25.000000               NaN       0.000000        0.000000   \n",
       "50%      NaN    31.000000               NaN       0.000000        0.000000   \n",
       "75%      NaN    42.000000               NaN       0.000000        0.000000   \n",
       "max      NaN    96.000000               NaN      10.000000       13.000000   \n",
       "\n",
       "        juv_other_count  priors_count  c_charge_degree  two_year_recid  \\\n",
       "count       5388.000000   5388.000000      5388.000000     5388.000000   \n",
       "unique              NaN           NaN              NaN             NaN   \n",
       "top                 NaN           NaN              NaN             NaN   \n",
       "freq                NaN           NaN              NaN             NaN   \n",
       "mean           0.112101      3.048998         0.612287        0.455642   \n",
       "std            0.474775      4.664760         0.487274        0.498075   \n",
       "min            0.000000      0.000000         0.000000        0.000000   \n",
       "25%            0.000000      0.000000         0.000000        0.000000   \n",
       "50%            0.000000      1.000000         1.000000        0.000000   \n",
       "75%            0.000000      4.000000         1.000000        1.000000   \n",
       "max            9.000000     38.000000         1.000000        1.000000   \n",
       "\n",
       "       compas_screening_date  days_b_screening_arrest c_offense_date  \\\n",
       "count                   5388              5388.000000           5388   \n",
       "unique                   678                      NaN            736   \n",
       "top               2013-04-20                      NaN     2013-01-14   \n",
       "freq                      25                      NaN             24   \n",
       "mean                     NaN                -1.612843            NaN   \n",
       "std                      NaN                 4.716708            NaN   \n",
       "min                      NaN               -30.000000            NaN   \n",
       "25%                      NaN                -1.000000            NaN   \n",
       "50%                      NaN                -1.000000            NaN   \n",
       "75%                      NaN                -1.000000            NaN   \n",
       "max                      NaN                30.000000            NaN   \n",
       "\n",
       "        c_days_from_compas  type_of_assessment  v_type_of_assessment  \\\n",
       "count          5388.000000              5388.0                5388.0   \n",
       "unique                 NaN                 NaN                   NaN   \n",
       "top                    NaN                 NaN                   NaN   \n",
       "freq                   NaN                 NaN                   NaN   \n",
       "mean             17.465293                 1.0                   1.0   \n",
       "std             229.852096                 0.0                   0.0   \n",
       "min               0.000000                 1.0                   1.0   \n",
       "25%               1.000000                 1.0                   1.0   \n",
       "50%               1.000000                 1.0                   1.0   \n",
       "75%               1.000000                 1.0                   1.0   \n",
       "max            9485.000000                 1.0                   1.0   \n",
       "\n",
       "       screening_date v_screening_date  days_in_jail  days_in_custody  \n",
       "count            5388             5388   5388.000000      5388.000000  \n",
       "unique            678              678           NaN              NaN  \n",
       "top        2013-04-20       2013-04-20           NaN              NaN  \n",
       "freq               25               25           NaN              NaN  \n",
       "mean              NaN              NaN     13.452858        33.182442  \n",
       "std               NaN              NaN     42.930678       169.105520  \n",
       "min               NaN              NaN      0.000000         0.000000  \n",
       "25%               NaN              NaN      1.000000         1.000000  \n",
       "50%               NaN              NaN      1.000000         2.000000  \n",
       "75%               NaN              NaN      5.000000        15.000000  \n",
       "max               NaN              NaN    749.000000      6035.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL N°1.4 : FEATURE ENGINEERING\n",
    "\n",
    "@pre:  A pandas.DataFrame df containing the dataset\n",
    "@post: A pandas.DataFrame df containing the previous dataset with the new features you created.\n",
    "\"\"\"\n",
    "\n",
    "# Convertir les colonnes datetime en objets datetime (si ce n'est pas déjà fait)\n",
    "df['c_jail_in'] = pd.to_datetime(df['c_jail_in'], errors='coerce')\n",
    "df['c_jail_out'] = pd.to_datetime(df['c_jail_out'], errors='coerce')\n",
    "df['in_custody'] = pd.to_datetime(df['in_custody'], errors='coerce')\n",
    "df['out_custody'] = pd.to_datetime(df['out_custody'], errors='coerce')\n",
    "\n",
    "# Calculer le nombre de jours passés en prison (c_jail_in à c_jail_out)\n",
    "df['days_in_jail'] = (df['c_jail_out'] - df['c_jail_in']).dt.days.astype(int)\n",
    "\n",
    "# Calculer le nombre de jours passés en garde à vue (in_custody à out_custody)\n",
    "df['days_in_custody'] = (df['out_custody'] - df['in_custody']).dt.days.astype(int)\n",
    "\n",
    "# Remplacer les valeurs NaN (si la date de sortie est manquante, cela signifie que la personne est toujours en garde à vue)\n",
    "df['days_in_jail'].fillna(0, inplace=True)\n",
    "df['days_in_custody'].fillna(0, inplace=True)\n",
    "\n",
    "# Vérification des nouvelles colonnes ajoutées\n",
    "df[['c_jail_in', 'c_jail_out', 'days_in_jail', 'in_custody', 'out_custody', 'days_in_custody']].head()\n",
    "\n",
    "# Suprrimer les colonnes (maintenant) inutiles\n",
    "df.drop(columns='c_jail_in', inplace=True)\n",
    "df.drop(columns='c_jail_out', inplace=True)\n",
    "df.drop(columns='in_custody', inplace=True)\n",
    "df.drop(columns='out_custody', inplace=True)\n",
    "\n",
    "# Mapper les données qualitatives -> quantitatives\n",
    "c_charge_degree_map = {\n",
    "    \"M\" : 0,\n",
    "    \"F\" : 1,\n",
    "}\n",
    "df['c_charge_degree'] = df['c_charge_degree'].map(c_charge_degree_map)\n",
    "\n",
    "\n",
    "\n",
    "v_type_of_assessment_map = {\n",
    "    \"Risk of Violence\" : 1,\n",
    "}\n",
    "\n",
    "df['v_type_of_assessment'] = df['v_type_of_assessment'].map(v_type_of_assessment_map)\n",
    "\n",
    "type_of_assessment_map = {\n",
    "    \"Risk of Recidivism\" : 1,\n",
    "}\n",
    "\n",
    "df['type_of_assessment'] = df['type_of_assessment'].map(type_of_assessment_map)\n",
    "\n",
    "# Infos\n",
    "df.info()\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the two features \"type_of_assessment\" and \"v_type_of_assessment\" have everywhere the same value : 1.\n",
    "As the type of assessment is alwyas \"Risk of recidivism\" and the violence type of assessment is always \"Risk of violence\", these two features do not add anything to the sample. We delete these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"type_of_assessment\"],inplace=True)\n",
    "df.drop(columns=[\"v_type_of_assessment\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.3] New features </b>  <br>\n",
    "What features have you added? If a particular manipulation has been applied, please explain.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.5 - Sensitive features</b> <br>\n",
    "</font>\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark 1.4] Sensitive features</b> <br>\n",
    "At this stage of the Hackathon, you still have two sensitive features, the sex attribute and the race attribute. As the end goal is to build a fair learning algorithm, you should not reasonably use these two features to determine if there is a risk of recidivism of the defendant.\n",
    "</div>\n",
    "\n",
    "To check if your learning techniques are unfair to particular subgroups of these features, you should **remove both features from the dataset while keeping them aside** to analyze the fairness of our learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5388 entries, 0 to 7213\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   age                      5388 non-null   int64  \n",
      " 1   juv_fel_count            5388 non-null   int64  \n",
      " 2   juv_misd_count           5388 non-null   int64  \n",
      " 3   juv_other_count          5388 non-null   int64  \n",
      " 4   priors_count             5388 non-null   int64  \n",
      " 5   c_charge_degree          5388 non-null   int64  \n",
      " 6   compas_screening_date    5388 non-null   object \n",
      " 7   days_b_screening_arrest  5388 non-null   float64\n",
      " 8   c_offense_date           5388 non-null   object \n",
      " 9   c_days_from_compas       5388 non-null   float64\n",
      " 10  screening_date           5388 non-null   object \n",
      " 11  v_screening_date         5388 non-null   object \n",
      " 12  days_in_jail             5388 non-null   int32  \n",
      " 13  days_in_custody          5388 non-null   int32  \n",
      "dtypes: float64(2), int32(2), int64(6), object(4)\n",
      "memory usage: 589.3+ KB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL N°1.5 : SENSITIVE FEATURES\n",
    "\n",
    "@pre:  A pandas.DataFrame df containing the dataset with sensitive features\n",
    "@post: A pandas.DataFrame df containing the dataset without sensitive features and separate numpy arrays for each sensitive feature (race and sex)\n",
    "as well as the true label array y.\n",
    "\"\"\"\n",
    "\n",
    "# Use the dictionaries below to encode numerically the sensitive features.\n",
    "race_map = {\n",
    "    \"African-American\": 0,\n",
    "    \"Caucasian\": 1,\n",
    "    \"Asian\": 2,\n",
    "    \"Other\": 3,\n",
    "    \"Hispanic\": 4,\n",
    "    \"Native American\": 5,\n",
    "}\n",
    "race = np.array([race_map[r] for r in df['race']])\n",
    "df.drop(columns=['race'], inplace=True)\n",
    "\n",
    "sex_map = {\"Male\": 0, \"Female\": 1}\n",
    "sex = np.array([sex_map[s] for s in df['sex']])\n",
    "df = df.drop(columns=['sex'])\n",
    "\n",
    "# As you approach the last step of the preprocessing, you should also store the target variable and remove it from the dataframe.\n",
    "# It is good practice to remove it just before the scaling so that its dimension corresponds to the number of data points in df.\n",
    "y = df[\"two_year_recid\"].astype(float).values\n",
    "df.drop(columns=[\"two_year_recid\"], inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.6 - Scaling the dataset</b> <br>\n",
    "</font>\n",
    "\n",
    "***Standardizing*** is important when you work with data because it allows data to be compared with one another.\n",
    "\n",
    "$z$ is the standard score of a population $x$. It can be computed as follows:\n",
    "$$z = \\frac{x-\\mu}{\\sigma}$$\n",
    "with $\\mu$ the mean of the population and $\\sigma$ the standard deviation of the population.\n",
    "\n",
    "Please consult [Wikipedia](https://en.wikipedia.org/wiki/Standard_score) for further information about the standardization.\\\n",
    "Be careful to use the same formula as us, check in `scikit-learn` and check the already existing imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5388 entries, 0 to 5387\n",
      "Data columns (total 10 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   age                      5388 non-null   float64\n",
      " 1   juv_fel_count            5388 non-null   float64\n",
      " 2   juv_misd_count           5388 non-null   float64\n",
      " 3   juv_other_count          5388 non-null   float64\n",
      " 4   priors_count             5388 non-null   float64\n",
      " 5   c_charge_degree          5388 non-null   float64\n",
      " 6   days_b_screening_arrest  5388 non-null   float64\n",
      " 7   c_days_from_compas       5388 non-null   float64\n",
      " 8   days_in_jail             5388 non-null   float64\n",
      " 9   days_in_custody          5388 non-null   float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 421.1 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <th>c_days_from_compas</th>\n",
       "      <th>days_in_jail</th>\n",
       "      <th>days_in_custody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.388000e+03</td>\n",
       "      <td>5.388000e+03</td>\n",
       "      <td>5.388000e+03</td>\n",
       "      <td>5.388000e+03</td>\n",
       "      <td>5.388000e+03</td>\n",
       "      <td>5.388000e+03</td>\n",
       "      <td>5.388000e+03</td>\n",
       "      <td>5.388000e+03</td>\n",
       "      <td>5.388000e+03</td>\n",
       "      <td>5.388000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.729591e-17</td>\n",
       "      <td>-4.639941e-16</td>\n",
       "      <td>1.250031e-16</td>\n",
       "      <td>-2.489141e-16</td>\n",
       "      <td>-8.416307e-17</td>\n",
       "      <td>5.803532e-16</td>\n",
       "      <td>1.032206e-16</td>\n",
       "      <td>-5.027221e-17</td>\n",
       "      <td>1.644008e-16</td>\n",
       "      <td>-1.796025e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000093e+00</td>\n",
       "      <td>1.000093e+00</td>\n",
       "      <td>1.000093e+00</td>\n",
       "      <td>1.000093e+00</td>\n",
       "      <td>1.000093e+00</td>\n",
       "      <td>1.000093e+00</td>\n",
       "      <td>1.000093e+00</td>\n",
       "      <td>1.000093e+00</td>\n",
       "      <td>1.000093e+00</td>\n",
       "      <td>1.000093e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.320304e+00</td>\n",
       "      <td>-1.318291e-01</td>\n",
       "      <td>-1.775688e-01</td>\n",
       "      <td>-2.361358e-01</td>\n",
       "      <td>-6.536843e-01</td>\n",
       "      <td>-1.256672e+00</td>\n",
       "      <td>-6.018984e+00</td>\n",
       "      <td>-7.599197e-02</td>\n",
       "      <td>-3.133914e-01</td>\n",
       "      <td>-1.962415e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.102510e-01</td>\n",
       "      <td>-1.318291e-01</td>\n",
       "      <td>-1.775688e-01</td>\n",
       "      <td>-2.361358e-01</td>\n",
       "      <td>-6.536843e-01</td>\n",
       "      <td>-1.256672e+00</td>\n",
       "      <td>1.299424e-01</td>\n",
       "      <td>-7.164095e-02</td>\n",
       "      <td>-2.900959e-01</td>\n",
       "      <td>-1.903275e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.001976e-01</td>\n",
       "      <td>-1.318291e-01</td>\n",
       "      <td>-1.775688e-01</td>\n",
       "      <td>-2.361358e-01</td>\n",
       "      <td>-4.392911e-01</td>\n",
       "      <td>7.957526e-01</td>\n",
       "      <td>1.299424e-01</td>\n",
       "      <td>-7.164095e-02</td>\n",
       "      <td>-2.900959e-01</td>\n",
       "      <td>-1.844135e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.349004e-01</td>\n",
       "      <td>-1.318291e-01</td>\n",
       "      <td>-1.775688e-01</td>\n",
       "      <td>-2.361358e-01</td>\n",
       "      <td>2.038884e-01</td>\n",
       "      <td>7.957526e-01</td>\n",
       "      <td>1.299424e-01</td>\n",
       "      <td>-7.164095e-02</td>\n",
       "      <td>-1.969138e-01</td>\n",
       "      <td>-1.075313e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.225381e+00</td>\n",
       "      <td>2.667176e+01</td>\n",
       "      <td>2.674371e+01</td>\n",
       "      <td>1.872197e+01</td>\n",
       "      <td>7.493256e+00</td>\n",
       "      <td>7.957526e-01</td>\n",
       "      <td>6.702933e+00</td>\n",
       "      <td>4.119350e+01</td>\n",
       "      <td>1.713496e+01</td>\n",
       "      <td>3.549485e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age  juv_fel_count  juv_misd_count  juv_other_count  \\\n",
       "count  5.388000e+03   5.388000e+03    5.388000e+03     5.388000e+03   \n",
       "mean  -3.729591e-17  -4.639941e-16    1.250031e-16    -2.489141e-16   \n",
       "std    1.000093e+00   1.000093e+00    1.000093e+00     1.000093e+00   \n",
       "min   -1.320304e+00  -1.318291e-01   -1.775688e-01    -2.361358e-01   \n",
       "25%   -8.102510e-01  -1.318291e-01   -1.775688e-01    -2.361358e-01   \n",
       "50%   -3.001976e-01  -1.318291e-01   -1.775688e-01    -2.361358e-01   \n",
       "75%    6.349004e-01  -1.318291e-01   -1.775688e-01    -2.361358e-01   \n",
       "max    5.225381e+00   2.667176e+01    2.674371e+01     1.872197e+01   \n",
       "\n",
       "       priors_count  c_charge_degree  days_b_screening_arrest  \\\n",
       "count  5.388000e+03     5.388000e+03             5.388000e+03   \n",
       "mean  -8.416307e-17     5.803532e-16             1.032206e-16   \n",
       "std    1.000093e+00     1.000093e+00             1.000093e+00   \n",
       "min   -6.536843e-01    -1.256672e+00            -6.018984e+00   \n",
       "25%   -6.536843e-01    -1.256672e+00             1.299424e-01   \n",
       "50%   -4.392911e-01     7.957526e-01             1.299424e-01   \n",
       "75%    2.038884e-01     7.957526e-01             1.299424e-01   \n",
       "max    7.493256e+00     7.957526e-01             6.702933e+00   \n",
       "\n",
       "       c_days_from_compas  days_in_jail  days_in_custody  \n",
       "count        5.388000e+03  5.388000e+03     5.388000e+03  \n",
       "mean        -5.027221e-17  1.644008e-16    -1.796025e-16  \n",
       "std          1.000093e+00  1.000093e+00     1.000093e+00  \n",
       "min         -7.599197e-02 -3.133914e-01    -1.962415e-01  \n",
       "25%         -7.164095e-02 -2.900959e-01    -1.903275e-01  \n",
       "50%         -7.164095e-02 -2.900959e-01    -1.844135e-01  \n",
       "75%         -7.164095e-02 -1.969138e-01    -1.075313e-01  \n",
       "max          4.119350e+01  1.713496e+01     3.549485e+01  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL N°1.6 : SCALE THE DATASET\n",
    "\n",
    "@pre:  A pandas.DataFrame df containing the dataset\n",
    "@post: A pandas.DataFrame df containing the standardized dataset\n",
    "\"\"\"\n",
    "\n",
    "# We decided to definitely remove date features before the scaling : \n",
    "\n",
    "to_remove = ['compas_screening_date', 'c_offense_date', 'screening_date',\n",
    "'v_screening_date']\n",
    "\n",
    "df = df.drop(columns=to_remove)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_dataset(df):\n",
    "    # Initialiser le scaler\n",
    "    scaler = StandardScaler()\n",
    "        \n",
    "    # Appliquer le scaler sur les colonnes numériques\n",
    "    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Appliquer la mise à l'échelle\n",
    "df = scale_dataset(df)\n",
    "\n",
    "# Afficher des informations après la normalisation\n",
    "df.info()\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 2 - Data Exploration</b> </font> <br><br>\n",
    "\n",
    "<font size=5 color=#009999> <b>2.1 - Feature visualization</b> <br>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the dataset balanced in terms of sensitive groups ?\n",
    "It's good practice to check this to better understand the contents of our dataset.\n",
    "Indeed, if the training dataset is severely imbalanced, our learning algorithm may perform better for over-represented groups than for under-represented groups. Moreover, our goal is for the model to perform equally well across all groups.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 2.1] (Im)Balanced dataset ? </b>  <br>\n",
    "Is the dataset imbalanced ? What could be the consequences in terms of fairness i.e. in terms of the model performing equally well across all groups ?\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL N°2.1.1: (Im)Balanced dataset ?\n",
    "\n",
    "@pre:  A pandas.DataFrame `X` containing the dataset\n",
    "@post: A pie chart plot representing the repartition of race groups in the dataset.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "pd.Series.plot\n",
    "X = df\n",
    "\n",
    "def show_pieplot(X) :\n",
    "\n",
    "    # As the race group was previously removed, we can temporarily add it back, using the following map.\n",
    "    race_reverse_map = {v: k for k, v in race_map.items()}\n",
    "\n",
    "    # Temporarily add the race group to the X dataframe\n",
    "    X['race'] = [race_reverse_map[r] for r in race]\n",
    "\n",
    "    # Plotting the pie chart\n",
    "    fig = plt.figure()\n",
    "    plt.title('Repartition of ethnical groups in the dataset', x=1, fontsize=10)\n",
    "    X['race'].value_counts().plot(kind='pie',\n",
    "                                  autopct='%1.1f%%', \n",
    "                                  ylabel='' ,\n",
    "                                  textprops={'fontsize': 8},\n",
    "                                  explode=(0,0,0,0,1.5,1.5)\n",
    "                                  )\n",
    "    fig.savefig('Q2pie.pdf',bbox_inches='tight')\n",
    "\n",
    "    # Removing the race group again after doing the plot\n",
    "    X.drop(columns=[\"race\"], inplace=True)\n",
    "\n",
    "show_pieplot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "In order to check to the important features in our dataset, we can compute and plot (see e.g. `sns.heatmap`) the correlation matrix, as a tool to visually show all the correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°2.1.2 : Correlation matrix\n",
    "\n",
    "@pre:  A pandas.DataFrame `df` containing the dataset\n",
    "@post: A visualization of the correlation matrix between features.\n",
    "\"\"\"\n",
    "\n",
    "def show_corr(X) :\n",
    "    fig = plt.figure()\n",
    "    plt.title('Correlation matrix of the dataset relevant features', fontsize=14, x=0.5, pad=10)\n",
    "    corr_matrix = X.corr(method='pearson')\n",
    "    sns.heatmap(corr_matrix,cmap='coolwarm',vmin=-1,vmax=1)\n",
    "    fig.savefig(\"Q2corr.pdf\",bbox_inches='tight')\n",
    "    fig.show()\n",
    "\n",
    "show_corr(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>2.2 Principal Component Analysis</b> <br>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is often considered as the simplest and most fundamental technique used in dimensionality reduction. Remember that PCA is essentially the rotation of coordinate axes, chosen such that each successful axis captures or preserves as much variance as possible. If the algorithm returns a new system coordinates of the same dimension as the input, we can keep only the axis corresponding to the 3 largest singular values and project data on this coordinates system to perform the visualization.\n",
    "\n",
    "To vizualize the importance of features, we can extract the PCA loadings. These are indicators of the correlation between components and original features. The value of loadings is contained between -1 and 1. The more the value goes toward those boundaries, the more the feature influences the choice of component.We propose to perform a 2-dimensional PCA and then to add the loadings in vector form to the figure to obtain what is called a biplot.\n",
    "\n",
    "The biplot visualization function is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°2.2.1 : Principal Component Analysis (2D)\n",
    "\n",
    "@pre:  A pandas.DataFrame `X` containing the dataset and labels `y`\n",
    "@post: A PCA visualization in 2D where points are colored with respect to true labels `y`\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def biplot_visualization(X, y, columns=None):\n",
    "    \"\"\"\n",
    "    Plot a biplot graph: the scaled data after applying a 2D PCA with loadings in vector forms.\n",
    "\n",
    "    :param pca: PCA object\n",
    "    :param X: a n by m matrix (or DataFrame), containing the input prior to the PCA transformation\n",
    "    :param y: a vector of length n containing the target\n",
    "    :param columns: a list of length m contained the names of the columns\n",
    "        If not given, X.columns will be used\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=2)\n",
    "    X = pca.fit_transform(X)\n",
    "\n",
    "    columns = (\n",
    "        columns\n",
    "        if columns is not None\n",
    "        else X.columns\n",
    "        if isinstance(X, pd.DataFrame)\n",
    "        else [f\"Feature {i+1}\" for i in range(X.shape[1])]\n",
    "    )\n",
    "\n",
    "    # Normalize data for scaling\n",
    "    X_normalized = X / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "    df = pd.DataFrame(data=X_normalized, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "    # Prepare loadings (vector components)\n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "    loadings_df = pd.DataFrame(loadings, columns=[\"PC1\", \"PC2\"], index=columns)\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=df[\"PC1\"], y=df[\"PC2\"], \n",
    "                    hue=y, \n",
    "                    palette={0:'blue',1:'black'}, \n",
    "                    s=50, alpha=0.7)\n",
    "\n",
    "    # Add vectors for loadings\n",
    "    for index, row in loadings_df.iterrows():\n",
    "        plt.arrow(\n",
    "            0,\n",
    "            0,\n",
    "            row.PC1,\n",
    "            row.PC2,\n",
    "            color=\"red\",\n",
    "            alpha=0.7,\n",
    "            head_width=0.02,\n",
    "            head_length=0.03,\n",
    "        )\n",
    "        plt.text(\n",
    "            row.PC1 * 1.1,\n",
    "            row.PC2 * 1.1,\n",
    "            index,\n",
    "            color=\"black\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    # Labels and limits\n",
    "    plt.title(\"2-components PCA applied to the dataset features\", fontsize=14)\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.axvline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend(title=\"two_years_recid\", loc=\"best\")\n",
    "    plt.savefig(\"Q2pca2.pdf\",bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "biplot_visualization(X, y, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, you are asked to perform a 3 components PCA and plot it using Plotly.\n",
    "<div class=\"alert alert-danger\">\n",
    " Note: On certain versions of Firefox, the 3D scatter function of plotly may have some issues.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°2.2.2 : Principal Component Analysis (3D)\n",
    "\n",
    "@pre:  A pandas.DataFrame `X` containing the dataset and labels `y`\n",
    "@post: A PCA visualization in 3D where points are colored with respect to true labels `y`\n",
    "\"\"\"\n",
    "\n",
    "def PCA_3comp_visu(X):\n",
    "\n",
    "    pca = PCA(n_components=3) # create the scikit-learn estimator, here the PCA with 3 PCs\n",
    "    X = pca.fit_transform(X) # fit the data, i.e., actually run the estimation of PCs and loadings from dataset X\n",
    "    X_normalized = X / (X.max(axis=0) - X.min(axis=0))\n",
    "    X = pd.DataFrame(data=X_normalized, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig = px.scatter_3d(X, x=\"PC1\", y=\"PC2\", z=\"PC3\", \n",
    "                        color=y, \n",
    "                        color_continuous_scale=['blue','black'],\n",
    "                        size=10*np.ones_like(X['PC1']), opacity=1,\n",
    "                        labels={'color':'two_years_recid'})\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "            aspectmode = 'data'\n",
    "        ),\n",
    "        width=1200, height=800,\n",
    "        showlegend = True\n",
    "        )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "PCA_3comp_visu(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 2.2] Principal Component Analysis </b>  <br>\n",
    "Do all features have the same importance? If no, which features are less important, and why? You can use all other graphs from the visualization part to justify your answer.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 3 - Clustering</b> </font> <br><br>\n",
    "\n",
    "<font size=4 color=#009999> <b>ABCs of Clustering</b> <br>\n",
    "Clustering can be defined as the task of *grouping* objects from a set $S$ (here, each row/observation is an object) in such a way that objects assigned to the same group (called cluster) are more **similar** (or less **distant**) with respect to each other (in some sense) than to those assigned to the other groups. Usually, we would like to divide our objects into $K$ groups.\n",
    "\n",
    "As such, clustering reduces to finding, among all $K$-partitions possible of $S$, the partition $\\mathcal{P}$ that minimizes some error criterion $f(\\mathcal{P})$. Each object will be assigned a cluster, $C_i$, and each cluster will have its centroid $c_i$ the distance between **any object** in $C_i$ to centroid $c_i$ is **always smaller** that the distance to any other centroid. In other words, each object is assigned to the cluster whose centroid is the closest.\n",
    "\n",
    "\n",
    "A mathematical formulation of the problem could be the following, $$ \\boxed{\\min_{(C_1,\\dots,C_K) \\,\\in\\, \\mathcal{P}}\\,f(C_1,\\dots,C_K) = \\sum_{i = 1}^{K}\\,\\sum_{x \\in C_i}\\,\\Delta(x,c_i)}$$\n",
    "\n",
    "where $\\Delta(x,c_i)$ denotes the distance between object $x$ and centroid $c_i$.\n",
    "\n",
    "<br>\n",
    "<font size=5 color=#009999>\n",
    "EXAMPLE OF SEPARATING OBJECTS INTO 10 CLUSTERS\n",
    "</font> <br> <br>\n",
    "\n",
    "**First**, let us imagine the following 2D dataset.\n",
    "\n",
    "<img src=\"Imgs/10-partitions-data.svg\" width = \"250\">\n",
    "\n",
    "**Then**, a 10-partition is defined by the position of the centroids, one for each cluster. Below, you can observe four examples of (random) centroids localizations (stars).\n",
    "\n",
    "<img src=\"Imgs/10-partitions-chose-centroids.svg\" width = \"1000\">\n",
    "\n",
    "**Next**, the regions are colored based on their closest centroid. Here, we take the distance to be the Euclidean distance.\n",
    "\n",
    "<img src=\"Imgs/10-partitions-centroids.svg\" width = \"1000\">\n",
    "\n",
    "**Finally**, data points (objects) are colored based in the region they are in.\n",
    "\n",
    "<img src=\"Imgs/10-partitions-clusters.svg\" width = \"1000\">\n",
    "\n",
    "<font size=5 color=#009999> <b>3.1 - K-Means</b> <br>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3.1.1 : GROUND TRUTH\n",
    "\n",
    "@pre:  A pandas.DataFrame `X` containing the dataset and labels `y`\n",
    "@post: A 80/20 split of your dataset in train and test sets.\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 3.1] Number of clusters </b>  <br>\n",
    "    Accounting for all features, what do you think is the ideal number of clusters? What will happen if too many or even too few clusters are chosen?\n",
    "</div>\n",
    "\n",
    "Now that your dataset is divided into a train and a test set, use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">KMeans</a> algorithm from `scikit-learn` to apply the clustering on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3.1.2 : K-Means\n",
    "\n",
    "@pre:  A split of your dataset: X_train, X_test, y_train, y_test\n",
    "@post: A split of your dataset in train and test sets.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, silhouette_score\n",
    "\n",
    "def train_and_predict(model, X_train, X_test):\n",
    "    \"\"\"Trains the clustering model on the training data and predict the clusters for both training and test data.\n",
    "\n",
    "    Parameters:\n",
    "    model (sklearn or similar clustering model): The clustering algorithm that has a fit_predict method and a predict method.\n",
    "    X_train (array-like, shape (n_samples, n_features)): The training data to fit the model on.\n",
    "    X_test (array-like, shape (n_samples, n_features)): The test data to predict the clusters for.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two arrays:\n",
    "        - train_clusters (array): Cluster labels for the training data.\n",
    "        - test_clusters (array): Cluster labels for the test data.\n",
    "    \"\"\"\n",
    "    train_clusters = model.fit_predict(X_train)\n",
    "    test_clusters = model.predict(X_test)       \n",
    "    return train_clusters, test_clusters\n",
    "\n",
    "\n",
    "def compute_y_pred(model, X_train, X_test, y_train):\n",
    "    \"\"\"Compute the predicted labels for the test data based on the clustering model.\n",
    "\n",
    "    This function assigns a predicted label to each sample in the test set by:\n",
    "    1. Training the model on the training data using the previous function.\n",
    "    2. Assigning the majority class from the training labels to each cluster.\n",
    "    3. Using the cluster assignments from the test data to assign predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    model (sklearn or similar clustering model): The trained clustering model with an `n_clusters` attribute.\n",
    "    X_train (array-like, shape (n_samples, n_features)): The training data used to fit the model.\n",
    "    X_test (array-like, shape (n_samples, n_features)): The test data to predict labels for.\n",
    "    y_train (array-like, shape (n_samples,)): The true labels of the training data.\n",
    "\n",
    "    Returns:\n",
    "    np.array: An array of predicted labels for the test data based on the majority class in each cluster.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    train_clusters, test_clusters = train_and_predict(model, X_train, X_test)\n",
    "    df = pd.DataFrame({\"cluster\": train_clusters, \"target\": y_train})\n",
    "\n",
    "    for cluster in range(model.n_clusters):\n",
    "        majority_class = df[df[\"cluster\"] == cluster][\"target\"].mode()[0]\n",
    "        mapping[cluster] = majority_class\n",
    "\n",
    "    y_pred = [mapping[cluster] for cluster in test_clusters]\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def compute_metrics(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Computes various evaluation metrics for the clustering model.\n",
    "\n",
    "    Parameters:\n",
    "    model (sklearn or similar clustering model): The trained clustering model with an `n_clusters` attribute.\n",
    "    X_train (array-like, shape (n_samples, n_features)): The training data used to fit the model.\n",
    "    X_test (array-like, shape (n_samples, n_features)): The test data to predict labels for.\n",
    "    y_train (array-like, shape (n_samples,)): The true labels of the training data.\n",
    "    y_test (array-like, shape (n_samples,)): The true labels of the test data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the computed metrics:\n",
    "        - \"n_clusters\": The number of clusters in the model.\n",
    "        - \"Accuracy\": The accuracy of the model on the test data.\n",
    "        - \"F1-Score\": The F1-score of the model on the test data.\n",
    "        - \"Precision\": The precision of the model on the test data.\n",
    "        - \"Recall\": The recall of the model on the test data.\n",
    "        - \"Silhouette Score\": The silhouette score of the clustering on the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_purity_entropy(clusters, y_test, n_clusters):\n",
    "        cluster_classes = pd.DataFrame({'cluster': clusters, 'true_class': y_test})\n",
    "        total_points = len(y_test)\n",
    "        purity = 0\n",
    "        entropy = 0\n",
    "\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_points = cluster_classes[cluster_classes['cluster'] == cluster]\n",
    "            n_points = len(cluster_points)\n",
    "            if n_points == 0:\n",
    "                continue  # Avoid division by zero\n",
    "\n",
    "            class_counts = cluster_points['true_class'].value_counts()\n",
    "            max_class_count = class_counts.max()\n",
    "            cluster_purity = max_class_count / n_points\n",
    "            purity += cluster_purity * n_points / total_points\n",
    "\n",
    "            cluster_entropy = 0\n",
    "            for count in class_counts:\n",
    "                p_ij = count / n_points\n",
    "                cluster_entropy -= p_ij * np.log2(p_ij)\n",
    "            entropy += cluster_entropy * n_points / total_points\n",
    "\n",
    "        return purity, entropy\n",
    "    \n",
    "    y_pred = compute_y_pred(model, X_train, X_test, y_train)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    sil_score = silhouette_score(X_test, model.predict(X_test))\n",
    "    test_clusters = model.predict(X_test)\n",
    "    purity, entropy = compute_purity_entropy(test_clusters, y_test, model.n_clusters)\n",
    "    return {\n",
    "        \"n_clusters\": model.n_clusters,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Silhouette Score\": sil_score,\n",
    "        \"Purity\": purity,\n",
    "        \"Entropy\": entropy,\n",
    "    }\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=random_seed)\n",
    "results = compute_metrics(kmeans, X_train, y_train, X_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>3.2 - Results Analysis</b> <br>\n",
    "</font>\n",
    "\n",
    "In this section, we adress the difficult task of evaluating the performance of the clustering algorithm.\n",
    "\n",
    "<font size=3 color=#009999> <b>3.2.1 - Quality of the clustering</b> <br>\n",
    "</font>\n",
    "The silhouette score is a measure of how close each point in one cluster is to points in the neighboring clusters. The [mean silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) is an average of the silhouette score for each point and provides a way to measure the quality of the clustering.\n",
    "\n",
    "The best value is 1 and the worst value is -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3.2.1 : Silhouette Score\n",
    "\n",
    "@pre:  A split of your dataset: X_train, X_test, y_train, y_test\n",
    "@post: A \"Mean Silhouette Score versus Number of Clusters\" plot\n",
    "\"\"\"\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2, 100) :\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    clusters = model.fit_predict(X_train)\n",
    "    score = silhouette_score(X_train, clusters)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 100), silhouette_scores, marker='o', linestyle='-', color='b')\n",
    "plt.title(\"Mean Silhouette Score versus Number of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Mean Silhouette Score\")\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.grid() \n",
    "\n",
    "plt.show()\n",
    "\n",
    "#################################################\n",
    "# Proposition : reprendre le code donné\n",
    "# Le code donné évalue la silhouette_score sur X_test et non sur X_train (ce qui est logique, en un sens ?)\n",
    "# A prendre avec des pincettes, j'ai p-ê rien compris\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2, 100) :\n",
    "    model = KMeans(n_clusters=k, random_state=random_seed)\n",
    "    results = compute_metrics(model, X_train, y_train, X_test, y_test)\n",
    "    silhouette_scores.append(results['Silhouette Score'])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 100), silhouette_scores, marker='o', linestyle='-', color='b')\n",
    "plt.title(\"Mean Silhouette Score versus Number of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Mean Silhouette Score\")\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.grid() \n",
    "\n",
    "plt.show()\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>3.2.2 - Purity and entropy of a clustering</b> <br>\n",
    "</font>\n",
    "\n",
    "### Purity\n",
    "\n",
    "Purity measures how well a cluster contains points from a single class. A cluster with high purity mostly contains points from one class.\n",
    "\n",
    "**Example:** Imagine you are grouping fruits based on their shape, but you also have information about their color. If a group contains mostly red apples, that group has high purity. However, if you find a few green apples or pears in the group, the purity decreases. In this case, high purity means the majority of fruits share both shape and color consistency.\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\text{Purity } = \\frac{1}{N} \\sum_{i = 1}^k \\max_j n_{i,j}\n",
    "$$\n",
    "where:\n",
    "- $N = $ total number of points,\n",
    "- $k = $ number of clusters,\n",
    "- $n_{i,j} = $​ number of points from class $j$ in cluster $i$,\n",
    "- $\\max_j n_{i,j} = $ number of points from the most common class in cluster $i$.\n",
    "\n",
    "\n",
    "### Entropy\n",
    "\n",
    "Entropy measures how mixed the classes are within a cluster. Low entropy means most points in a cluster belong to the same class. High entropy means points are more evenly distributed across different classes.\n",
    "\n",
    "**Example:** Consider a fruit basket that is mostly filled with red apples, with only a few bananas and oranges. Since the basket is dominated by one type of fruit, it has low entropy. In contrast, if the basket contains an equal mix of apples, bananas, and oranges, the distribution is more random, resulting in high entropy. This even distribution means it is harder to predict the dominant fruit just by looking at the basket.\n",
    "\n",
    "***Formula for a single cluster:***\n",
    "$$\n",
    "E_i = -\\sum_{j=1}^{C} p_{ij} \\log_2(p_{ij})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $C = $ number of classes,\n",
    "- $p_{i,j} = $ proportion of points from class jj in cluster ii.\n",
    "\n",
    "The overall entropy is the weighted average across all clusters:\n",
    "\n",
    "$$\n",
    "\\text{Entropy} = \\frac{1}{N} \\sum_{i=1}^{k} n_i \\cdot E_i\n",
    "$$\n",
    "\n",
    "Where $n_i$​ is the number of points in cluster $i$.\n",
    "\n",
    "A good clustering aims for both high purity (most points in a cluster belong to one class) and low entropy (each cluster contains little class mixing).\n",
    "    \n",
    "<div class=\"alert alert-danger\">\n",
    " If this makes it easier for you to implement purity and entropy, you can modify the previously defined function `compute_metrics` to also return in the results dictionary the purity, the entropy or any other metric that you may want to use later on.\n",
    "</div>\n",
    "<div class=\"alert alert-danger\">\n",
    " Compared to the silhouette score which is computed using only the features, purity and entropy are metrics computed using the true label `y`. Do not forget to compute these metrics on a test set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3.2.2 : Purity and Entropy\n",
    "\n",
    "@pre:  A split of your dataset: X_train, X_test, y_train, y_test\n",
    "@post: A \"Purity/Entropy versus Number of Clusters\" plot. There should be two curves, one for the purity and one for the entropy.\n",
    "\"\"\"\n",
    "\n",
    "purity_scores = []\n",
    "entropy_scores = []\n",
    "\n",
    "\n",
    "for n_clusters in range(2, 100):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    metrics = compute_metrics(kmeans, X_train, y_train, X_test, y_test)\n",
    "    purity_scores.append(metrics[\"Purity\"])\n",
    "    entropy_scores.append(metrics[\"Entropy\"])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 100), purity_scores, label=\"Purity\", marker=\"o\")\n",
    "plt.plot(range(2, 100), entropy_scores, label=\"Entropy\", marker=\"o\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Purity and Entropy versus Number of Clusters\")\n",
    "plt.legend()\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 3.2] Quality of the clustering </b>  <br>\n",
    "    You considered three different measures for the quality of the clustering: the first one is the silhouette score and is oblivious to the true labels: it is a truly unsupervised metric. The second and third metric use the true label to assess the quality of the clustering. Based on this observation,\n",
    "    \n",
    "1. Comment on the evolution of each metric according to the number of clusters.\n",
    "2. Comment on what do you now think is the ideal number of clusters ?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 4 - Fairness metrics</b> </font> <br><br>\n",
    "\n",
    "Congratulations for reaching this far ! So far, you have thoroughly analyzed a sensitive dataset, you cleaned it and focused on what you believe were useful features for predicting recidivism. You then used the K-Means algorithm to have your own recidivism predictor.\n",
    "\n",
    "Because of the sensitivity of the dataset and its potential negative impact on certain parts of the population, you should now assess its fairness with respect to each gender and race group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>4.1 False Positive Rate</b> <br>\n",
    "</font>\n",
    "\n",
    "The false positive rate (FPR) is a performance metric used to evaluate the accuracy of a machine learning model, particularly in binary classification tasks. It refers to the proportion of actual negative instances (people that did not recidivate) that are incorrectly classified as positive. A lower FPR indicates that the model is better at identifying negative cases.\n",
    "\n",
    "A fair model would have the same FPR across all groups.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    " As for the purity and entropy metrics, the false positive rate metric uses the true labels, you should therefore make a train/test split before hand.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "CELL N°4.1 False Positive Rate\n",
    "\n",
    "@pre:  A split of your dataset: X_train, X_test, y_train, y_test\n",
    "@post: A \"False Positive Rate vs Number of Clusters\" plot for each group\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Because the dataset is imbalanced, we will repartition our dataset into three race groups: African-American, Caucasian and Other.\n",
    "\n",
    "# Doing so, you can now use X_test[group_val == i] to get the test points with race i.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example placeholders for your dataset variables\n",
    "# Replace these with your actual dataset and features\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "# Step 1: Repartition dataset into racial groups\n",
    "\n",
    "group_labels = np.where(race == 0, 0, np.where(race == 1, 1, 2))\n",
    "# Step 2: Train/test split\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    X, y, group_labels, test_size=0.2, random_state=random_seed\n",
    ")\n",
    "print(group_test)\n",
    "\n",
    "# Step 3: Define FPR computation function\n",
    "def compute_fpr(y_true, y_pred):\n",
    "    \"\"\"Compute the false positive rate.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    if cm.size == 4:  # Cas normal\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:  # Gestion des cas avec des classes manquantes\n",
    "        tn, fp, fn, tp = 0, 0, 0, 0\n",
    "        if cm.shape == (1, 2):  # Une seule ligne (tous les labels vrais sont 0)\n",
    "            tn, fp = cm[0]\n",
    "        elif cm.shape == (2, 1):  # Une seule colonne (tous les labels prédits sont 0)\n",
    "            fn, tp = cm[:, 0]\n",
    "    return fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "\n",
    "# Step 4: Range of clusters\n",
    "num_clusters = range(2, 11)\n",
    "fpr_results = {0: [], 1: [], 2: []}  # For African-American, Caucasian, Other\n",
    "\n",
    "# Step 5: K-Means clustering and FPR calculation for each group\n",
    "for k in num_clusters:\n",
    "    y_pred = compute_y_pred(KMeans(n_clusters=k, random_state=random_seed), X_train, X_test, y_train)\n",
    "    # Compute FPR for each group\n",
    "    for group in [0, 1, 2]:\n",
    "        group_mask = group_test == group\n",
    "        \n",
    "\n",
    "        print(group_mask)\n",
    "        y_test = np.array(y_test)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_true_group = y_test[group_mask]\n",
    "        y_pred_group = y_pred[group_mask]\n",
    "        \n",
    "        \n",
    "\n",
    "        fpr = compute_fpr(y_true_group, y_pred_group)\n",
    "        fpr_results[group].append(fpr)\n",
    "\n",
    "# Step 6: Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['blue', 'green', 'orange']\n",
    "labels = ['African-American', 'Caucasian', 'Other']\n",
    "\n",
    "for group, color, label in zip(fpr_results.keys(), colors, labels):\n",
    "    plt.plot(num_clusters, fpr_results[group], marker='o', color=color, label=label)\n",
    "\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"False Positive Rate\")\n",
    "plt.title(\"False Positive Rate vs. Number of Clusters\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>4.2 Demographic Parity</b> <br>\n",
    "</font>\n",
    "Demographic parity is a fairness metric aimed at ensuring that a machine learning model’s predictions do not depend on membership in a sensitive group. Specifically, demographic parity is achieved when the likelihood of a prediction is independent of sensitive group membership. In binary classification, demographic parity requires equal selection rates across groups.\n",
    "\n",
    "In our case, perfect demographic parity means that there is the exact same proportion of “bail denied” in each race group. A fair model would have the same Demographic Parity value across all groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fonction pour calculer la parité démographique\n",
    "def compute_demographic_parity(y_pred, group_labels):\n",
    "    \"\"\"Calcul de la parité démographique pour chaque groupe.\"\"\"\n",
    "    unique_groups = np.unique(group_labels)\n",
    "    demographic_parity = {}\n",
    "\n",
    "    for group in unique_groups:\n",
    "        group_mask = group_labels == group\n",
    "\n",
    "      \n",
    "        group_pred = y_pred[group_mask]\n",
    "        positive_rate = np.mean(group_pred == 1)  \n",
    "\n",
    "        demographic_parity[group] = positive_rate\n",
    "\n",
    "    return demographic_parity\n",
    "\n",
    "# Paramètres\n",
    "cluster_range = range(2, 11)  \n",
    "demographic_parity_diff = [] \n",
    "\n",
    "for n_clusters in cluster_range:\n",
    "    # Étape 1 : Appliquer K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(X_train)\n",
    "\n",
    "    cluster_labels = kmeans.predict(X_train)\n",
    "\n",
    "    y_pred = cluster_labels % 2\n",
    "\n",
    "    dp = compute_demographic_parity(y_pred, group_train)\n",
    "\n",
    "    # Étape 4 : Calcul de la différence maximale entre les groupes\n",
    "    max_diff = max(dp.values()) - min(dp.values())\n",
    "    demographic_parity_diff.append(max_diff)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cluster_range, demographic_parity_diff, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Demographic Parity Difference\")\n",
    "plt.title(\"Demographic Parity vs. Number of Clusters\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 4.1] Fairness of your model </b>  <br>\n",
    "    You considered two different measures for the fairness of your model and checked for various variants of your algorithm (number of clusters) the value of these fairness metrics.\n",
    "\n",
    "Is your algorithm unfair ? If yes, which ethnic group is penalized by the unfairness of your model ?\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 4.2] Presence of the sensitive features in the dataset [BONUS]</b> <br> \n",
    "In Cell 1.5, you removed the sensitive features from your dataset before building your algorithm. Yet, you may have noticed unfairness in your algorithm.\n",
    "\n",
    "1. Provide reasons why it is not necessarily enough to remove sensitive features from your dataset if you want to have fair predictions.\n",
    "2. Compute FPR and Demographic Parity for your algorithm when trained on the full dataset. Is the fairness of your classifier worse ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for the BONUS question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 5 - Visualization </b> </font> <br><br>\n",
    "\n",
    "<font size=5 color=#009999> <b>5.1 Visualize your results</b> <br>\n",
    "</font>\n",
    "In the last cell, you can create the figure of your choice to visualize your results. You can be as creative as you want as long as you only use one figure (with potentially more than one plot).\n",
    "\n",
    "You will be evaluated on the clarity of your figure. You should ask yourself the following question while creating it: \"Is the message I am trying to convey clear enough so that a student from another group can take a quick look and understand it directly ?\" If the answer is positive, it's probably a great plot !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for the VISUALIZATION question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
